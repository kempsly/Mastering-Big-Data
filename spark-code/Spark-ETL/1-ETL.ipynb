{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, col, explode\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/29 10:12:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# # Extract\n",
    "# spark = SparkSession.builder.appName(\"ETLPipeline\").getOrCreate()\n",
    "# df = spark.read.text(\"Word_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PostgreSQL_PySpark\") \\\n",
    "    .config(\"spark.jars\", \"/opt/homebrew/lib/postgresql-42.6.0.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.text(\"Word_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|This is a Japanes...|\n",
      "|The team members ...|\n",
      "|As the years pass...|\n",
      "|If you don't like...|\n",
      "|He was disappoint...|\n",
      "|When he encounter...|\n",
      "|Situps are a terr...|\n",
      "|Toddlers feeding ...|\n",
      "|Edith could decid...|\n",
      "|Her daily goal wa...|\n",
      "|Tomorrow will bri...|\n",
      "|His son quipped t...|\n",
      "|He wondered why a...|\n",
      "|If my calculator ...|\n",
      "|The hummingbird's...|\n",
      "|He went on a whis...|\n",
      "|This is the last ...|\n",
      "|I come from a tri...|\n",
      "|The delicious aro...|\n",
      "|Weather is not tr...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|               value|         splitedData|\n",
      "+--------------------+--------------------+\n",
      "|This is a Japanes...|[This, is, a, Jap...|\n",
      "|The team members ...|[The, team, membe...|\n",
      "|As the years pass...|[As, the, years, ...|\n",
      "|If you don't like...|[If, you, don't, ...|\n",
      "|He was disappoint...|[He, was, disappo...|\n",
      "|When he encounter...|[When, he, encoun...|\n",
      "|Situps are a terr...|[Situps, are, a, ...|\n",
      "|Toddlers feeding ...|[Toddlers, feedin...|\n",
      "|Edith could decid...|[Edith, could, de...|\n",
      "|Her daily goal wa...|[Her, daily, goal...|\n",
      "|Tomorrow will bri...|[Tomorrow, will, ...|\n",
      "|His son quipped t...|[His, son, quippe...|\n",
      "|He wondered why a...|[He, wondered, wh...|\n",
      "|If my calculator ...|[If, my, calculat...|\n",
      "|The hummingbird's...|[The, hummingbird...|\n",
      "|He went on a whis...|[He, went, on, a,...|\n",
      "|This is the last ...|[This, is, the, l...|\n",
      "|I come from a tri...|[I, come, from, a...|\n",
      "|The delicious aro...|[The, delicious, ...|\n",
      "|Weather is not tr...|[Weather, is, not...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumn(\"splitedData\", f.split(\"value\",\" \"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------+\n",
      "|               value|         splitedData|   words|\n",
      "+--------------------+--------------------+--------+\n",
      "|This is a Japanes...|[This, is, a, Jap...|    This|\n",
      "|This is a Japanes...|[This, is, a, Jap...|      is|\n",
      "|This is a Japanes...|[This, is, a, Jap...|       a|\n",
      "|This is a Japanes...|[This, is, a, Jap...|Japanese|\n",
      "|This is a Japanes...|[This, is, a, Jap...|    doll|\n",
      "|The team members ...|[The, team, membe...|     The|\n",
      "|The team members ...|[The, team, membe...|    team|\n",
      "|The team members ...|[The, team, membe...| members|\n",
      "|The team members ...|[The, team, membe...|    were|\n",
      "|The team members ...|[The, team, membe...|    hard|\n",
      "|The team members ...|[The, team, membe...|      to|\n",
      "|The team members ...|[The, team, membe...|    tell|\n",
      "|The team members ...|[The, team, membe...|   apart|\n",
      "|The team members ...|[The, team, membe...|   since|\n",
      "|The team members ...|[The, team, membe...|    they|\n",
      "|The team members ...|[The, team, membe...|     all|\n",
      "|The team members ...|[The, team, membe...|    wore|\n",
      "|The team members ...|[The, team, membe...|   their|\n",
      "|The team members ...|[The, team, membe...|    hair|\n",
      "|The team members ...|[The, team, membe...|      in|\n",
      "+--------------------+--------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df3 = df2.withColumn(\"words\", explode(\"splitedData\"))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|   words|\n",
      "+--------+\n",
      "|    This|\n",
      "|      is|\n",
      "|       a|\n",
      "|Japanese|\n",
      "|    doll|\n",
      "|     The|\n",
      "|    team|\n",
      "| members|\n",
      "|    were|\n",
      "|    hard|\n",
      "|      to|\n",
      "|    tell|\n",
      "|   apart|\n",
      "|   since|\n",
      "|    they|\n",
      "|     all|\n",
      "|    wore|\n",
      "|   their|\n",
      "|    hair|\n",
      "|      in|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordsDF = df3.select(\"words\")\n",
    "wordsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordCount = wordsDF.groupBy(\"words\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|      words|count|\n",
      "+-----------+-----+\n",
      "|   Tomorrow|    4|\n",
      "|         If|    8|\n",
      "|      leave|    4|\n",
      "|      corny|    4|\n",
      "|        day|    4|\n",
      "|preoccupied|    4|\n",
      "|       even|    8|\n",
      "|      crazy|    4|\n",
      "|    bananas|    4|\n",
      "|     priest|    4|\n",
      "|        did|    4|\n",
      "|    whether|    4|\n",
      "|     Having|    4|\n",
      "|        I'm|    4|\n",
      "|      crime|    4|\n",
      "|  surprised|    4|\n",
      "|      James|    4|\n",
      "|      could|    8|\n",
      "|        buy|    4|\n",
      "|        him|    8|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordCount.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, col, explode\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load to PostgreSQL\n",
    "driver = \"org.postgresql.Driver\"\n",
    "url = \"database-1.cvq2m462i7av.us-east-2.rds.amazonaws.com/\"  # Add the database name\n",
    "table = \"kps_schema_pyspark.WordCount\"\n",
    "user = \"postgres\"\n",
    "password = \"abc12345678\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write the wordcount\n",
    "wordCount.write.format(\"jdbc\") \\\n",
    "    .option(\"driver\", driver) \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"dbtable\", table) \\\n",
    "    .option(\"mode\", \"append\") \\\n",
    "    .option(\"user\", user) \\\n",
    "    .option(\"password\", password) \\\n",
    "    .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Set up the Spark session\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"ETL Pipeline\") \\\n",
    "#     .config(\"spark.jars\", \"/path/to/postgresql-42.6.0.jar\")  # Ensure the JDBC driver is added here\n",
    "#     .getOrCreate()\n",
    "\n",
    "# # Extract data (Make sure you specify the correct path for the file)\n",
    "# file_path = \"./WordData.txt\"  # Replace with the actual path to the file\n",
    "# df = spark.read.text(file_path)\n",
    "\n",
    "# # Transformation\n",
    "# df2 = df.withColumn(\"splitedData\", f.split(\"value\", \" \"))  # Split the data by space\n",
    "# df3 = df2.withColumn(\"words\", explode(\"splitedData\"))  # Explode the array to individual words\n",
    "# wordsDF = df3.select(\"words\")\n",
    "# wordCount = wordsDF.groupBy(\"words\").count()  # Count the occurrences of each word\n",
    "\n",
    "\n",
    "\n",
    "# wordCount.write.format(\"jdbc\") \\\n",
    "#     .option(\"driver\", driver) \\\n",
    "#     .option(\"url\", url) \\\n",
    "#     .option(\"dbtable\", table) \\\n",
    "#     .option(\"mode\", \"append\") \\\n",
    "#     .option(\"user\", user) \\\n",
    "#     .option(\"password\", password) \\\n",
    "#     .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
