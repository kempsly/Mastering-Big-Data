{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import statemenets\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS \n",
    "from pyspark.ml.evaluation import RegressionEvaluator \n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator \n",
    "from pyspark.sql.functions import col, explode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/28 14:23:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"CollaborativeFilteringRecsEngine\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://client-172-18-120-79.eduroam.universite-paris-saclay.fr:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>CollaborativeFilteringRecsEngine</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x10eb43830>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the datasets\n",
    "moviesDF = spark.read.options(header=\"True\", inferSchema=\"True\").csv(\"./data/movies.csv\")\n",
    "ratingsDF = spark.read.options(header=\"True\", inferSchema=\"True\").csv(\"./data/rating.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|movieId|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
      "|      5|Father of the Bri...|              Comedy|\n",
      "|      6|         Heat (1995)|Action|Crime|Thri...|\n",
      "|      7|      Sabrina (1995)|      Comedy|Romance|\n",
      "|      8| Tom and Huck (1995)|  Adventure|Children|\n",
      "|      9| Sudden Death (1995)|              Action|\n",
      "|     10|    GoldenEye (1995)|Action|Adventure|...|\n",
      "|     11|American Presiden...|Comedy|Drama|Romance|\n",
      "|     12|Dracula: Dead and...|       Comedy|Horror|\n",
      "|     13|        Balto (1995)|Adventure|Animati...|\n",
      "|     14|        Nixon (1995)|               Drama|\n",
      "|     15|Cutthroat Island ...|Action|Adventure|...|\n",
      "|     16|       Casino (1995)|         Crime|Drama|\n",
      "|     17|Sense and Sensibi...|       Drama|Romance|\n",
      "|     18|   Four Rooms (1995)|              Comedy|\n",
      "|     19|Ace Ventura: When...|              Comedy|\n",
      "|     20|  Money Train (1995)|Action|Comedy|Cri...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "moviesDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- genres: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "moviesDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|      1|   4.0|964982703|\n",
      "|     1|      3|   4.0|964981247|\n",
      "|     1|      6|   4.0|964982224|\n",
      "|     1|     47|   5.0|964983815|\n",
      "|     1|     50|   5.0|964982931|\n",
      "|     1|     70|   3.0|964982400|\n",
      "|     1|    101|   5.0|964980868|\n",
      "|     1|    110|   4.0|964982176|\n",
      "|     1|    151|   5.0|964984041|\n",
      "|     1|    157|   5.0|964984100|\n",
      "|     1|    163|   5.0|964983650|\n",
      "|     1|    216|   5.0|964981208|\n",
      "|     1|    223|   3.0|964980985|\n",
      "|     1|    231|   5.0|964981179|\n",
      "|     1|    235|   4.0|964980908|\n",
      "|     1|    260|   5.0|964981680|\n",
      "|     1|    296|   3.0|964982967|\n",
      "|     1|    316|   3.0|964982310|\n",
      "|     1|    333|   5.0|964981179|\n",
      "|     1|    349|   4.0|964982563|\n",
      "+------+-------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratingsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- timestamp: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratingsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[movieId: int, title: string, genres: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(moviesDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[userId: int, movieId: int, rating: double, timestamp: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(ratingsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing a join on the two datasets\n",
    "#We  join the 2 datasets on movieID columns, that is common between them, \n",
    "# we perform the left join\n",
    "#By doing so, we avoid films(movies) without rating\n",
    "ratings = ratingsDF.join(moviesDF, 'movieId', 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+---------+--------------------+--------------------+\n",
      "|movieId|userId|rating|timestamp|               title|              genres|\n",
      "+-------+------+------+---------+--------------------+--------------------+\n",
      "|      1|     1|   4.0|964982703|    Toy Story (1995)|Adventure|Animati...|\n",
      "|      3|     1|   4.0|964981247|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      6|     1|   4.0|964982224|         Heat (1995)|Action|Crime|Thri...|\n",
      "|     47|     1|   5.0|964983815|Seven (a.k.a. Se7...|    Mystery|Thriller|\n",
      "|     50|     1|   5.0|964982931|Usual Suspects, T...|Crime|Mystery|Thr...|\n",
      "|     70|     1|   3.0|964982400|From Dusk Till Da...|Action|Comedy|Hor...|\n",
      "|    101|     1|   5.0|964980868|Bottle Rocket (1996)|Adventure|Comedy|...|\n",
      "|    110|     1|   4.0|964982176|   Braveheart (1995)|    Action|Drama|War|\n",
      "|    151|     1|   5.0|964984041|      Rob Roy (1995)|Action|Drama|Roma...|\n",
      "|    157|     1|   5.0|964984100|Canadian Bacon (1...|          Comedy|War|\n",
      "|    163|     1|   5.0|964983650|    Desperado (1995)|Action|Romance|We...|\n",
      "|    216|     1|   5.0|964981208|Billy Madison (1995)|              Comedy|\n",
      "|    223|     1|   3.0|964980985|       Clerks (1994)|              Comedy|\n",
      "|    231|     1|   5.0|964981179|Dumb & Dumber (Du...|    Adventure|Comedy|\n",
      "|    235|     1|   4.0|964980908|      Ed Wood (1994)|        Comedy|Drama|\n",
      "|    260|     1|   5.0|964981680|Star Wars: Episod...|Action|Adventure|...|\n",
      "|    296|     1|   3.0|964982967| Pulp Fiction (1994)|Comedy|Crime|Dram...|\n",
      "|    316|     1|   3.0|964982310|     Stargate (1994)|Action|Adventure|...|\n",
      "|    333|     1|   5.0|964981179|    Tommy Boy (1995)|              Comedy|\n",
      "|    349|     1|   4.0|964982563|Clear and Present...|Action|Crime|Dram...|\n",
      "+-------+------+------+---------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratings.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data spliting (80%-20%)(Training, testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, test) = ratings.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100836"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80468\n",
      "+-------+------+------+----------+----------------+--------------------+\n",
      "|movieId|userId|rating| timestamp|           title|              genres|\n",
      "+-------+------+------+----------+----------------+--------------------+\n",
      "|      1|     1|   4.0| 964982703|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|     5|   4.0| 847434962|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|     7|   4.5|1106635946|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    18|   3.5|1455209816|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    19|   4.0| 965705637|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    21|   3.5|1407618878|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    27|   3.0| 962685262|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    33|   3.0| 939647444|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    40|   5.0| 832058959|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    43|   5.0| 848993983|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    44|   3.0| 869251860|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    45|   4.0| 951170182|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    46|   5.0| 834787906|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    50|   3.0|1514238116|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    54|   3.0| 830247330|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    57|   5.0| 965796031|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    63|   5.0|1443199669|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    64|   4.0|1161520134|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    66|   4.0|1104643957|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    68|   2.5|1158531426|Toy Story (1995)|Adventure|Animati...|\n",
      "+-------+------+------+----------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train.count())\n",
    "train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20368\n",
      "+-------+------+------+----------+----------------+--------------------+\n",
      "|movieId|userId|rating| timestamp|           title|              genres|\n",
      "+-------+------+------+----------+----------------+--------------------+\n",
      "|      1|    15|   2.5|1510577970|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    17|   4.5|1305696483|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    31|   5.0| 850466616|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    32|   3.0| 856736119|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    73|   4.5|1464196374|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    96|   5.0| 964772990|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|   103|   4.0|1431954238|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|   121|   4.0| 847656180|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|   132|   2.0|1157921785|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|   137|   4.0|1204859907|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|   160|   4.0| 971115026|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|   166|   5.0|1189980529|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|   179|   4.0| 852114051|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|   193|   2.0|1435856890|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|   201|   5.0| 939801780|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|   223|   3.5|1226209758|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|   229|   5.0| 838144316|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|   234|   5.0|1004409347|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|   254|   4.5|1180446553|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|   266|   2.0| 945669542|Toy Story (1995)|Adventure|Animati...|\n",
      "+-------+------+------+----------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(test.count())\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "als = ALS(userCol = \"userId\", itemCol=\"movieId\", ratingCol=\"rating\", nonnegative=True,implicitPrefs=False, coldStartStrategy=\"drop\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line of code is creating an **ALS (Alternating Least Squares)** model using PySpark's MLlib for building a recommendation system.\n",
    "\n",
    "### **Code Breakdown:**\n",
    "\n",
    "```python\n",
    "als = ALS(\n",
    "    userCol=\"userId\",\n",
    "    itemCol=\"movieId\",\n",
    "    ratingCol=\"rating\",\n",
    "    nonnegative=True,\n",
    "    implicitPrefs=False,\n",
    "    coldStartStrategy=\"drop\"\n",
    ")\n",
    "```\n",
    "\n",
    "#### **Parameters:**\n",
    "\n",
    "1. **`userCol=\"userId\"`**\n",
    "   - Specifies the column name in your DataFrame that contains the **user IDs**.\n",
    "   - Each unique user is identified by an ID.\n",
    "\n",
    "2. **`itemCol=\"movieId\"`**\n",
    "   - Specifies the column name that contains the **item IDs** (e.g., movies in this case).\n",
    "   - Each unique item is identified by an ID.\n",
    "\n",
    "3. **`ratingCol=\"rating\"`**\n",
    "   - Specifies the column name that contains the **ratings** (e.g., user feedback for items such as 1–5 stars).\n",
    "\n",
    "4. **`nonnegative=True`**\n",
    "   - Ensures that all matrix factorization values (user and item factors) are non-negative.\n",
    "   - Non-negative values make sense for many recommendation problems, like ratings or counts, where negative values would not be valid.\n",
    "\n",
    "5. **`implicitPrefs=False`**\n",
    "   - Indicates whether the model is for **explicit preferences** (e.g., numeric ratings like 1-5).\n",
    "   - If `True`, it assumes **implicit feedback** (e.g., clicks, views, or purchases) instead of explicit ratings.\n",
    "\n",
    "6. **`coldStartStrategy=\"drop\"`**\n",
    "   - Specifies how to handle **cold-start scenarios** (when the model encounters unseen users or items during predictions):\n",
    "     - `\"drop\"`: Drops predictions for cold-start users/items to avoid errors.\n",
    "     - Other options may allow fallback behavior, but `\"drop\"` is common.\n",
    "\n",
    "---\n",
    "\n",
    "### **How This Works in Practice:**\n",
    "\n",
    "This `ALS` instance is set up to:\n",
    "- Use user-item-rating data.\n",
    "- Predict explicit preferences (ratings).\n",
    "- Generate only valid, non-negative predictions.\n",
    "- Handle cold-start users or items by skipping their predictions.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ALS_36bad81e9267"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "als"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperParameters setup  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(als.rank, [10, 50, 100, 150]) \\\n",
    "            .addGrid(als.regParam, [.01, .05, .1, .15]) \\\n",
    "            .build()\n",
    "\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "           metricName=\"rmse\", \n",
    "           labelCol=\"rating\", \n",
    "           predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{Param(parent='ALS_36bad81e9267', name='rank', doc='rank of the factorization'): 10,\n",
       "  Param(parent='ALS_36bad81e9267', name='regParam', doc='regularization parameter (>= 0).'): 0.01},\n",
       " {Param(parent='ALS_36bad81e9267', name='rank', doc='rank of the factorization'): 10,\n",
       "  Param(parent='ALS_36bad81e9267', name='regParam', doc='regularization parameter (>= 0).'): 0.05},\n",
       " {Param(parent='ALS_36bad81e9267', name='rank', doc='rank of the factorization'): 10,\n",
       "  Param(parent='ALS_36bad81e9267', name='regParam', doc='regularization parameter (>= 0).'): 0.1},\n",
       " {Param(parent='ALS_36bad81e9267', name='rank', doc='rank of the factorization'): 10,\n",
       "  Param(parent='ALS_36bad81e9267', name='regParam', doc='regularization parameter (>= 0).'): 0.15},\n",
       " {Param(parent='ALS_36bad81e9267', name='rank', doc='rank of the factorization'): 50,\n",
       "  Param(parent='ALS_36bad81e9267', name='regParam', doc='regularization parameter (>= 0).'): 0.01},\n",
       " {Param(parent='ALS_36bad81e9267', name='rank', doc='rank of the factorization'): 50,\n",
       "  Param(parent='ALS_36bad81e9267', name='regParam', doc='regularization parameter (>= 0).'): 0.05},\n",
       " {Param(parent='ALS_36bad81e9267', name='rank', doc='rank of the factorization'): 50,\n",
       "  Param(parent='ALS_36bad81e9267', name='regParam', doc='regularization parameter (>= 0).'): 0.1},\n",
       " {Param(parent='ALS_36bad81e9267', name='rank', doc='rank of the factorization'): 50,\n",
       "  Param(parent='ALS_36bad81e9267', name='regParam', doc='regularization parameter (>= 0).'): 0.15},\n",
       " {Param(parent='ALS_36bad81e9267', name='rank', doc='rank of the factorization'): 100,\n",
       "  Param(parent='ALS_36bad81e9267', name='regParam', doc='regularization parameter (>= 0).'): 0.01},\n",
       " {Param(parent='ALS_36bad81e9267', name='rank', doc='rank of the factorization'): 100,\n",
       "  Param(parent='ALS_36bad81e9267', name='regParam', doc='regularization parameter (>= 0).'): 0.05},\n",
       " {Param(parent='ALS_36bad81e9267', name='rank', doc='rank of the factorization'): 100,\n",
       "  Param(parent='ALS_36bad81e9267', name='regParam', doc='regularization parameter (>= 0).'): 0.1},\n",
       " {Param(parent='ALS_36bad81e9267', name='rank', doc='rank of the factorization'): 100,\n",
       "  Param(parent='ALS_36bad81e9267', name='regParam', doc='regularization parameter (>= 0).'): 0.15},\n",
       " {Param(parent='ALS_36bad81e9267', name='rank', doc='rank of the factorization'): 150,\n",
       "  Param(parent='ALS_36bad81e9267', name='regParam', doc='regularization parameter (>= 0).'): 0.01},\n",
       " {Param(parent='ALS_36bad81e9267', name='rank', doc='rank of the factorization'): 150,\n",
       "  Param(parent='ALS_36bad81e9267', name='regParam', doc='regularization parameter (>= 0).'): 0.05},\n",
       " {Param(parent='ALS_36bad81e9267', name='rank', doc='rank of the factorization'): 150,\n",
       "  Param(parent='ALS_36bad81e9267', name='regParam', doc='regularization parameter (>= 0).'): 0.1},\n",
       " {Param(parent='ALS_36bad81e9267', name='rank', doc='rank of the factorization'): 150,\n",
       "  Param(parent='ALS_36bad81e9267', name='regParam', doc='regularization parameter (>= 0).'): 0.15}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegressionEvaluator_4b5cc2ba175e"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using cross validation\n",
    "cv = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrossValidator_c5745a811dde"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/28 14:23:42 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kempslysilencieux/Documents/bigdata-tuto/pyspark-env/lib/python3.12/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kempslysilencieux/Documents/bigdata-tuto/pyspark-env/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kempslysilencieux/Documents/bigdata-tuto/pyspark-env/lib/python3.12/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "# A fatal error has been detected by the Java Runtime Environment:\n",
      "#\n",
      "#  SIGSEGV (0xb) at pc=0x000000010aaa5150, pid=3477, tid=222979\n",
      "#\n",
      "# JRE version: OpenJDK Runtime Environment Homebrew (11.0.26) (build 11.0.26+0)\n",
      "# Java VM: OpenJDK 64-Bit Server VM Homebrew (11.0.26+0, mixed mode, tiered, compressed oops, g1 gc, bsd-aarch64)\n",
      "# Problematic frame:\n",
      "# V  [libjvm.dylib+0x695150]  ObjectSynchronizer::inflate(Thread*, oopDesc*, ObjectSynchronizer::InflateCause)+0x18c\n",
      "#\n",
      "# No core dump will be written. Core dumps have been disabled. To enable core dumping, try \"ulimit -c unlimited\" before starting Java again\n",
      "#\n",
      "# An error report file with more information is saved as:\n",
      "# /Users/kempslysilencieux/Documents/bigdata-tuto/spark-code/Spark-ML/Spark-collaborative-filtering/hs_err_pid3477.log\n",
      "[thread 225283 also had an error]\n",
      "[thread 28167 also had an error][thread 211971 also had an error]\n",
      "\n",
      "[thread 223747 also had an error][thread 224259 also had an error]\n",
      "\n",
      "#\n",
      "# If you would like to submit a bug report, please visit:\n",
      "#   https://github.com/Homebrew/homebrew-core/issues\n",
      "#\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling o203.fit",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/bigdata-tuto/pyspark-env/lib/python3.12/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/bigdata-tuto/pyspark-env/lib/python3.12/site-packages/pyspark/ml/tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    841\u001b[0m train \u001b[38;5;241m=\u001b[39m datasets[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[1;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubModel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimap_unordered\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetrics_all\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcollectSubModelsParam\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/pool.py:873\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 873\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/multiprocessing/pool.py:125\u001b[0m, in \u001b[0;36mworker\u001b[0;34m(inqueue, outqueue, initializer, initargs, maxtasks, wrap_exception)\u001b[0m\n\u001b[1;32m    123\u001b[0m job, i, func, args, kwds \u001b[38;5;241m=\u001b[39m task\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 125\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wrap_exception \u001b[38;5;129;01mand\u001b[39;00m func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _helper_reraises_exception:\n",
      "File \u001b[0;32m~/Documents/bigdata-tuto/pyspark-env/lib/python3.12/site-packages/pyspark/ml/tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit.<locals>.<lambda>\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    841\u001b[0m train \u001b[38;5;241m=\u001b[39m datasets[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[1;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, tasks):\n\u001b[1;32m    848\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[0;32m~/Documents/bigdata-tuto/pyspark-env/lib/python3.12/site-packages/pyspark/util.py:342\u001b[0m, in \u001b[0;36minheritable_thread_target.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    341\u001b[0m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_jsc\u001b[38;5;241m.\u001b[39msc()\u001b[38;5;241m.\u001b[39msetLocalProperties(properties)\n\u001b[0;32m--> 342\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/bigdata-tuto/pyspark-env/lib/python3.12/site-packages/pyspark/ml/tuning.py:113\u001b[0m, in \u001b[0;36m_parallelFitTasks.<locals>.singleTask\u001b[0;34m()\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msingleTask\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, Transformer]:\n\u001b[0;32m--> 113\u001b[0m     index, model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodelIter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# TODO: duplicate evaluator to take extra params from input\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;66;03m#  Note: Supporting tuning params in evaluator need update method\u001b[39;00m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m#  `MetaAlgorithmReadWrite.getAllNestedStages`, make it return\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m#  all nested stages and evaluators\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     metric \u001b[38;5;241m=\u001b[39m eva\u001b[38;5;241m.\u001b[39mevaluate(model\u001b[38;5;241m.\u001b[39mtransform(validation, epm[index]))\n",
      "File \u001b[0;32m~/Documents/bigdata-tuto/pyspark-env/lib/python3.12/site-packages/pyspark/ml/base.py:98\u001b[0m, in \u001b[0;36m_FitMultipleIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo models remaining.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcounter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m index, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfitSingleModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/bigdata-tuto/pyspark-env/lib/python3.12/site-packages/pyspark/ml/base.py:156\u001b[0m, in \u001b[0;36mEstimator.fitMultiple.<locals>.fitSingleModel\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfitSingleModel\u001b[39m(index: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m M:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparamMaps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/bigdata-tuto/pyspark-env/lib/python3.12/site-packages/pyspark/ml/base.py:203\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(params, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m params:\n\u001b[0;32m--> 203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n",
      "File \u001b[0;32m~/Documents/bigdata-tuto/pyspark-env/lib/python3.12/site-packages/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m~/Documents/bigdata-tuto/pyspark-env/lib/python3.12/site-packages/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/bigdata-tuto/pyspark-env/lib/python3.12/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/bigdata-tuto/pyspark-env/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/Documents/bigdata-tuto/pyspark-env/lib/python3.12/site-packages/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m                 \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;28mtype\u001b[39m \u001b[38;5;241m=\u001b[39m answer[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o203.fit"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kempslysilencieux/Documents/bigdata-tuto/pyspark-env/lib/python3.12/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/socket.py\", line 720, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [Errno 54] Connection reset by peer\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kempslysilencieux/Documents/bigdata-tuto/pyspark-env/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kempslysilencieux/Documents/bigdata-tuto/pyspark-env/lib/python3.12/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "model = cv.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = model.bestModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = best_model.transform(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = evaluator.evaluate(test_predictions)\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = best_model.recommendForAllUsers(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = recommendations\n",
    "\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.withColumn(\"movieid_rating\", explode(\"recommendations\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df2.select(\"userId\", col(\"movieid_rating.movieId\"), col(\"movieid_rating.rating\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/bigdata-tuto/pyspark-env/lib/python3.12/site-packages/py4j/clientserver.py:503\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 503\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msendall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mBrokenPipeError\u001b[0m: [Errno 32] Broken pipe",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/bigdata-tuto/pyspark-env/lib/python3.12/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n",
      "File \u001b[0;32m~/Documents/bigdata-tuto/pyspark-env/lib/python3.12/site-packages/py4j/clientserver.py:506\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    505\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while sending or receiving.\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 506\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JNetworkError(\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while sending\u001b[39m\u001b[38;5;124m\"\u001b[39m, e, proto\u001b[38;5;241m.\u001b[39mERROR_ON_SEND)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mPy4JNetworkError\u001b[0m: Error while sending",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m col, explode\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Initialize Spark session\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCollaborative filtering\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# COMMAND ----------\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Load movies and ratings data\u001b[39;00m\n\u001b[1;32m     16\u001b[0m moviesDF \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39moptions(header\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m\"\u001b[39m, inferSchema\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcsv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/movies.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/bigdata-tuto/pyspark-env/lib/python3.12/site-packages/pyspark/sql/session.py:503\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    500\u001b[0m     session \u001b[38;5;241m=\u001b[39m SparkSession(sc, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[0;32m--> 503\u001b[0m         \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSparkSession$\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMODULE$\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m     )\u001b[38;5;241m.\u001b[39mapplyModifiableSettings(session\u001b[38;5;241m.\u001b[39m_jsparkSession, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options)\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m session\n",
      "File \u001b[0;32m~/Documents/bigdata-tuto/pyspark-env/lib/python3.12/site-packages/py4j/java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m UserHelpAutoCompletion\u001b[38;5;241m.\u001b[39mKEY:\n\u001b[1;32m   1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[0;32m-> 1712\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, jvm_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n",
      "File \u001b[0;32m~/Documents/bigdata-tuto/pyspark-env/lib/python3.12/site-packages/py4j/java_gateway.py:1053\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(retry, connection, pne):\n\u001b[1;32m   1052\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException while sending command.\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 1053\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbinary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1055\u001b[0m     logging\u001b[38;5;241m.\u001b[39mexception(\n\u001b[1;32m   1056\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException while sending command.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/bigdata-tuto/pyspark-env/lib/python3.12/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m~/Documents/bigdata-tuto/pyspark-env/lib/python3.12/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/Documents/bigdata-tuto/pyspark-env/lib/python3.12/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m~/Documents/bigdata-tuto/pyspark-env/lib/python3.12/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.functions import col, explode\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"Collaborative filtering\").getOrCreate()\n",
    "\n",
    "\n",
    "# Load movies and ratings data\n",
    "moviesDF = spark.read.options(header=\"True\", inferSchema=\"True\").csv(\"./data/movies.csv\")\n",
    "ratingsDF = spark.read.options(header=\"True\", inferSchema=\"True\").csv(\"./data/rating.csv\")\n",
    "\n",
    "# Display the dataframes to ensure they are loaded correctly\n",
    "display(moviesDF)\n",
    "display(ratingsDF)\n",
    "\n",
    "\n",
    "# Join ratings with movies on movieId\n",
    "ratings = ratingsDF.join(moviesDF, 'movieId', 'left')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "(train, test) = ratings.randomSplit([0.8, 0.2])\n",
    "\n",
    "\n",
    "# Check the count of rows in the training dataset\n",
    "ratings.count()\n",
    "\n",
    "# Show some rows from the training dataset\n",
    "print(train.count())\n",
    "train.show()\n",
    "\n",
    "# Show some rows from the test dataset\n",
    "print(test.count())\n",
    "test.show()\n",
    "\n",
    "# Initialize the ALS model\n",
    "als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", nonnegative=True, implicitPrefs=False, coldStartStrategy=\"drop\")\n",
    "\n",
    "# Set up parameter grid for cross-validation\n",
    "param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(als.rank, [10, 50, 100, 150]) \\\n",
    "            .addGrid(als.regParam, [.01, .05, .1, .15]) \\\n",
    "            .build()\n",
    "\n",
    "# Initialize evaluator for RMSE\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\", \n",
    "    labelCol=\"rating\", \n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize CrossValidator\n",
    "cv = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Fit the model using cross-validation\n",
    "model = cv.fit(train)\n",
    "\n",
    "# Get the best model\n",
    "best_model = model.bestModel\n",
    "\n",
    "# Transform the test set using the best model\n",
    "test_predictions = best_model.transform(test)\n",
    "\n",
    "# Evaluate the RMSE on the test set\n",
    "RMSE = evaluator.evaluate(test_predictions)\n",
    "print(f\"RMSE: {RMSE}\")\n",
    "\n",
    "# Get recommendations for all users\n",
    "recommendations = best_model.recommendForAllUsers(5)\n",
    "\n",
    "# Display the recommendations dataframe\n",
    "display(recommendations)\n",
    "\n",
    "# Explode the recommendations array and select the relevant columns\n",
    "df2 = recommendations.withColumn(\"movieid_rating\", explode(\"recommendations\"))\n",
    "\n",
    "# Display the exploded dataframe with selected columns\n",
    "df2.select(\n",
    "    \"userId\", \n",
    "    col(\"movieid_rating.movieId\").alias(\"movieId\"), \n",
    "    col(\"movieid_rating.rating\").alias(\"rating\")\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/28 14:34:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+\n",
      "|movieId|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
      "|      5|Father of the Bri...|              Comedy|\n",
      "|      6|         Heat (1995)|Action|Crime|Thri...|\n",
      "|      7|      Sabrina (1995)|      Comedy|Romance|\n",
      "|      8| Tom and Huck (1995)|  Adventure|Children|\n",
      "|      9| Sudden Death (1995)|              Action|\n",
      "|     10|    GoldenEye (1995)|Action|Adventure|...|\n",
      "|     11|American Presiden...|Comedy|Drama|Romance|\n",
      "|     12|Dracula: Dead and...|       Comedy|Horror|\n",
      "|     13|        Balto (1995)|Adventure|Animati...|\n",
      "|     14|        Nixon (1995)|               Drama|\n",
      "|     15|Cutthroat Island ...|Action|Adventure|...|\n",
      "|     16|       Casino (1995)|         Crime|Drama|\n",
      "|     17|Sense and Sensibi...|       Drama|Romance|\n",
      "|     18|   Four Rooms (1995)|              Comedy|\n",
      "|     19|Ace Ventura: When...|              Comedy|\n",
      "|     20|  Money Train (1995)|Action|Comedy|Cri...|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|      1|   4.0|964982703|\n",
      "|     1|      3|   4.0|964981247|\n",
      "|     1|      6|   4.0|964982224|\n",
      "|     1|     47|   5.0|964983815|\n",
      "|     1|     50|   5.0|964982931|\n",
      "|     1|     70|   3.0|964982400|\n",
      "|     1|    101|   5.0|964980868|\n",
      "|     1|    110|   4.0|964982176|\n",
      "|     1|    151|   5.0|964984041|\n",
      "|     1|    157|   5.0|964984100|\n",
      "|     1|    163|   5.0|964983650|\n",
      "|     1|    216|   5.0|964981208|\n",
      "|     1|    223|   3.0|964980985|\n",
      "|     1|    231|   5.0|964981179|\n",
      "|     1|    235|   4.0|964980908|\n",
      "|     1|    260|   5.0|964981680|\n",
      "|     1|    296|   3.0|964982967|\n",
      "|     1|    316|   3.0|964982310|\n",
      "|     1|    333|   5.0|964981179|\n",
      "|     1|    349|   4.0|964982563|\n",
      "+------+-------+------+---------+\n",
      "only showing top 20 rows\n",
      "\n",
      "80822\n",
      "+-------+------+------+----------+----------------+--------------------+\n",
      "|movieId|userId|rating| timestamp|           title|              genres|\n",
      "+-------+------+------+----------+----------------+--------------------+\n",
      "|      1|     1|   4.0| 964982703|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|     5|   4.0| 847434962|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|     7|   4.5|1106635946|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    17|   4.5|1305696483|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    18|   3.5|1455209816|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    21|   3.5|1407618878|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    27|   3.0| 962685262|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    33|   3.0| 939647444|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    40|   5.0| 832058959|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    43|   5.0| 848993983|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    44|   3.0| 869251860|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    45|   4.0| 951170182|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    46|   5.0| 834787906|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    50|   3.0|1514238116|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    54|   3.0| 830247330|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    57|   5.0| 965796031|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    64|   4.0|1161520134|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    66|   4.0|1104643957|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    68|   2.5|1158531426|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    73|   4.5|1464196374|Toy Story (1995)|Adventure|Animati...|\n",
      "+-------+------+------+----------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "20014\n",
      "+-------+------+------+----------+----------------+--------------------+\n",
      "|movieId|userId|rating| timestamp|           title|              genres|\n",
      "+-------+------+------+----------+----------------+--------------------+\n",
      "|      1|    15|   2.5|1510577970|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    19|   4.0| 965705637|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    31|   5.0| 850466616|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    32|   3.0| 856736119|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    63|   5.0|1443199669|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|    71|   5.0| 864737933|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|   178|   4.0|1164354911|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|   179|   4.0| 852114051|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|   202|   4.0| 974923506|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|   214|   3.0| 853937855|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|   217|   4.0| 955942540|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|   234|   5.0|1004409347|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|   240|   5.0| 849122434|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|   270|   5.0| 853918728|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|   274|   4.0|1171410158|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|   275|   5.0|1049076484|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|   279|   3.0|1506394495|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|   323|   3.5|1422640363|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|   337|   4.0| 860255715|Toy Story (1995)|Adventure|Animati...|\n",
      "|      1|   347|   5.0| 847645986|Toy Story (1995)|Adventure|Animati...|\n",
      "+-------+------+------+----------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kempslysilencieux/Documents/bigdata-tuto/pyspark-env/lib/python3.12/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kempslysilencieux/Documents/bigdata-tuto/pyspark-env/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kempslysilencieux/Documents/bigdata-tuto/pyspark-env/lib/python3.12/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\n",
      "# A fatal error has been detected by the Java Runtime Environment:\n",
      "#\n",
      "#  SIGSEGV (0xb) at pc=0x000000010a2a5150, pid=3682, tid=197123\n",
      "#\n",
      "# JRE version: OpenJDK Runtime Environment Homebrew (11.0.26) (build 11.0.26+0)\n",
      "# Java VM: OpenJDK 64-Bit Server VM Homebrew (11.0.26+0, mixed mode, tiered, compressed oops, g1 gc, bsd-aarch64)\n",
      "# Problematic frame:\n",
      "# V  [libjvm.dylib+0x695150]  ObjectSynchronizer::inflate(Thread*, oopDesc*, ObjectSynchronizer::InflateCause)+0x18c\n",
      "#\n",
      "# No core dump will be written. Core dumps have been disabled. To enable core dumping, try \"ulimit -c unlimited\" before starting Java again\n",
      "#\n",
      "# An error report file with more information is saved as:\n",
      "# /Users/kempslysilencieux/Documents/bigdata-tuto/spark-code/Spark-ML/Spark-collaborative-filtering/hs_err_pid3682.log\n",
      "#\n",
      "# If you would like to submit a bug report, please visit:\n",
      "#   https://github.com/Homebrew/homebrew-core/issues\n",
      "#\n",
      "An error occurred: An error occurred while calling o196.fit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kempslysilencieux/Documents/bigdata-tuto/pyspark-env/lib/python3.12/site-packages/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/kempslysilencieux/Documents/bigdata-tuto/pyspark-env/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/kempslysilencieux/Documents/bigdata-tuto/pyspark-env/lib/python3.12/site-packages/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.functions import col, explode\n",
    "\n",
    "# Initialize Spark session with adjusted parameters\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Collaborative filtering\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.cores.max\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load movies and ratings data\n",
    "moviesDF = spark.read.options(header=\"True\", inferSchema=\"True\").csv(\"./data/movies.csv\")\n",
    "ratingsDF = spark.read.options(header=\"True\", inferSchema=\"True\").csv(\"./data/rating.csv\")\n",
    "\n",
    "# Display the dataframes to ensure they are loaded correctly\n",
    "moviesDF.show()\n",
    "ratingsDF.show()\n",
    "\n",
    "# Join ratings with movies on movieId\n",
    "ratings = ratingsDF.join(moviesDF, 'movieId', 'left')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "(train, test) = ratings.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Check the count of rows in the training dataset\n",
    "print(train.count())\n",
    "train.show()\n",
    "\n",
    "# Show some rows from the test dataset\n",
    "print(test.count())\n",
    "test.show()\n",
    "\n",
    "# Initialize the ALS model\n",
    "als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", nonnegative=True, implicitPrefs=False, coldStartStrategy=\"drop\")\n",
    "\n",
    "# Set up parameter grid for cross-validation\n",
    "param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(als.rank, [10, 50, 100, 150]) \\\n",
    "            .addGrid(als.regParam, [.01, .05, .1, .15]) \\\n",
    "            .build()\n",
    "\n",
    "# Initialize evaluator for RMSE\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\", \n",
    "    labelCol=\"rating\", \n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "# Initialize CrossValidator\n",
    "cv = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "# Fit the model using cross-validation\n",
    "try:\n",
    "    model = cv.fit(train)\n",
    "    # Get the best model\n",
    "    best_model = model.bestModel\n",
    "    # Transform the test set using the best model\n",
    "    test_predictions = best_model.transform(test)\n",
    "    # Evaluate the RMSE on the test set\n",
    "    RMSE = evaluator.evaluate(test_predictions)\n",
    "    print(f\"RMSE: {RMSE}\")\n",
    "    # Get recommendations for all users\n",
    "    recommendations = best_model.recommendForAllUsers(5)\n",
    "    # Display the recommendations dataframe\n",
    "    recommendations.show()\n",
    "    # Explode the recommendations array and select the relevant columns\n",
    "    df2 = recommendations.withColumn(\"movieid_rating\", explode(\"recommendations\"))\n",
    "    # Display the exploded dataframe with selected columns\n",
    "    df2.select(\n",
    "        \"userId\", \n",
    "        col(\"movieid_rating.movieId\").alias(\"movieId\"), \n",
    "        col(\"movieid_rating.rating\").alias(\"rating\")\n",
    "    ).show()\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/28 14:40:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/01/28 14:40:25 DEBUG FileSystem: Loading filesystems\n",
      "25/01/28 14:40:25 DEBUG FileSystem: nullscan:// = class org.apache.hadoop.hive.ql.io.NullScanFileSystem from /opt/spark/jars/hive-exec-2.3.9-core.jar\n",
      "25/01/28 14:40:25 DEBUG FileSystem: file:// = class org.apache.hadoop.fs.LocalFileSystem from /opt/spark/jars/hadoop-client-api-3.3.4.jar\n",
      "25/01/28 14:40:25 DEBUG FileSystem: file:// = class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem from /opt/spark/jars/hive-exec-2.3.9-core.jar\n",
      "25/01/28 14:40:25 DEBUG FileSystem: viewfs:// = class org.apache.hadoop.fs.viewfs.ViewFileSystem from /opt/spark/jars/hadoop-client-api-3.3.4.jar\n",
      "25/01/28 14:40:25 DEBUG FileSystem: har:// = class org.apache.hadoop.fs.HarFileSystem from /opt/spark/jars/hadoop-client-api-3.3.4.jar\n",
      "25/01/28 14:40:25 DEBUG FileSystem: http:// = class org.apache.hadoop.fs.http.HttpFileSystem from /opt/spark/jars/hadoop-client-api-3.3.4.jar\n",
      "25/01/28 14:40:25 DEBUG FileSystem: https:// = class org.apache.hadoop.fs.http.HttpsFileSystem from /opt/spark/jars/hadoop-client-api-3.3.4.jar\n",
      "25/01/28 14:40:25 DEBUG FileSystem: hdfs:// = class org.apache.hadoop.hdfs.DistributedFileSystem from /opt/spark/jars/hadoop-client-api-3.3.4.jar\n",
      "25/01/28 14:40:25 DEBUG FileSystem: webhdfs:// = class org.apache.hadoop.hdfs.web.WebHdfsFileSystem from /opt/spark/jars/hadoop-client-api-3.3.4.jar\n",
      "25/01/28 14:40:25 DEBUG FileSystem: swebhdfs:// = class org.apache.hadoop.hdfs.web.SWebHdfsFileSystem from /opt/spark/jars/hadoop-client-api-3.3.4.jar\n",
      "25/01/28 14:40:25 DEBUG FileSystem: Looking for FS supporting file\n",
      "25/01/28 14:40:25 DEBUG FileSystem: looking for configuration option fs.file.impl\n",
      "25/01/28 14:40:25 DEBUG FileSystem: Looking in service filesystems for implementation class\n",
      "25/01/28 14:40:25 DEBUG FileSystem: FS for file is class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem\n",
      "25/01/28 14:40:25 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "25/01/28 14:40:25 DEBUG SharedState: Applying other initial session options to HadoopConf: spark.app.name -> Collaborative Filtering\n",
      "25/01/28 14:40:25 DEBUG SharedState: Applying other initial session options to HadoopConf: spark.driver.memory -> 2g\n",
      "25/01/28 14:40:25 DEBUG SharedState: Applying other initial session options to HadoopConf: spark.default.parallelism -> 4\n",
      "25/01/28 14:40:25 DEBUG SharedState: Applying other initial session options to HadoopConf: spark.executor.memory -> 2g\n",
      "25/01/28 14:40:25 DEBUG SharedState: Applying other initial session options to HadoopConf: spark.sql.shuffle.partitions -> 4\n",
      "25/01/28 14:40:25 DEBUG FileSystem: Starting: Acquiring creator semaphore for file:/Users/kempslysilencieux/Documents/bigdata-tuto/spark-code/Spark-ML/Spark-collaborative-filtering/spark-warehouse\n",
      "25/01/28 14:40:25 DEBUG FileSystem: Acquiring creator semaphore for file:/Users/kempslysilencieux/Documents/bigdata-tuto/spark-code/Spark-ML/Spark-collaborative-filtering/spark-warehouse: duration 0:00.000s\n",
      "25/01/28 14:40:25 DEBUG FileSystem: Starting: Creating FS file:/Users/kempslysilencieux/Documents/bigdata-tuto/spark-code/Spark-ML/Spark-collaborative-filtering/spark-warehouse\n",
      "25/01/28 14:40:25 DEBUG FileSystem: Looking for FS supporting file\n",
      "25/01/28 14:40:25 DEBUG FileSystem: looking for configuration option fs.file.impl\n",
      "25/01/28 14:40:25 DEBUG FileSystem: Looking in service filesystems for implementation class\n",
      "25/01/28 14:40:25 DEBUG FileSystem: FS for file is class org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem\n",
      "25/01/28 14:40:25 DEBUG FileSystem: Creating FS file:/Users/kempslysilencieux/Documents/bigdata-tuto/spark-code/Spark-ML/Spark-collaborative-filtering/spark-warehouse: duration 0:00.002s\n",
      "25/01/28 14:40:25 INFO SharedState: Warehouse path is 'file:/Users/kempslysilencieux/Documents/bigdata-tuto/spark-code/Spark-ML/Spark-collaborative-filtering/spark-warehouse'.\n",
      "25/01/28 14:40:25 DEBUG FsUrlStreamHandlerFactory: Creating handler for protocol jar\n",
      "25/01/28 14:40:25 DEBUG FileSystem: Looking for FS supporting jar\n",
      "25/01/28 14:40:25 DEBUG FileSystem: looking for configuration option fs.jar.impl\n",
      "25/01/28 14:40:25 DEBUG FileSystem: Looking in service filesystems for implementation class\n",
      "25/01/28 14:40:25 DEBUG FsUrlStreamHandlerFactory: Unknown protocol jar, delegating to default implementation\n",
      "25/01/28 14:40:25 DEBUG DataSource: Some paths were ignored:\n",
      "  \n",
      "25/01/28 14:40:25 INFO InMemoryFileIndex: It took 15 ms to list leaf files for 1 paths.\n",
      "25/01/28 14:40:25 INFO InMemoryFileIndex: It took 0 ms to list leaf files for 1 paths.\n",
      "25/01/28 14:40:26 DEBUG CatalystSqlParser: Parsing command: spark_grouping_id\n",
      "25/01/28 14:40:26 DEBUG Analyzer$ResolveReferences: Resolving 'value to value#0\n",
      "25/01/28 14:40:26 DEBUG Analyzer$ResolveReferences: Resolving 'value to value#7\n",
      "25/01/28 14:40:26 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/01/28 14:40:26 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n",
      "25/01/28 14:40:26 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 011 */\n",
      "/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 013 */     this.references = references;\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 017 */     partitionIndex = index;\n",
      "/* 018 */     this.inputs = inputs;\n",
      "/* 019 */     inputadapter_input_0 = inputs[0];\n",
      "/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 021 */\n",
      "/* 022 */   }\n",
      "/* 023 */\n",
      "/* 024 */   protected void processNext() throws java.io.IOException {\n",
      "/* 025 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 027 */\n",
      "/* 028 */       do {\n",
      "/* 029 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 030 */         UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 031 */         null : (inputadapter_row_0.getUTF8String(0));\n",
      "/* 032 */\n",
      "/* 033 */         boolean filter_isNull_0 = true;\n",
      "/* 034 */         boolean filter_value_0 = false;\n",
      "/* 035 */         boolean filter_isNull_2 = false;\n",
      "/* 036 */         UTF8String filter_value_2 = null;\n",
      "/* 037 */         if (inputadapter_isNull_0) {\n",
      "/* 038 */           filter_isNull_2 = true;\n",
      "/* 039 */         } else {\n",
      "/* 040 */           filter_value_2 = inputadapter_value_0.trim();\n",
      "/* 041 */         }\n",
      "/* 042 */         boolean filter_isNull_1 = filter_isNull_2;\n",
      "/* 043 */         int filter_value_1 = -1;\n",
      "/* 044 */\n",
      "/* 045 */         if (!filter_isNull_2) {\n",
      "/* 046 */           filter_value_1 = (filter_value_2).numChars();\n",
      "/* 047 */         }\n",
      "/* 048 */         if (!filter_isNull_1) {\n",
      "/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.\n",
      "/* 050 */           filter_value_0 = filter_value_1 > 0;\n",
      "/* 051 */\n",
      "/* 052 */         }\n",
      "/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;\n",
      "/* 054 */\n",
      "/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 056 */\n",
      "/* 057 */         filter_mutableStateArray_0[0].reset();\n",
      "/* 058 */\n",
      "/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 060 */\n",
      "/* 061 */         if (inputadapter_isNull_0) {\n",
      "/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 063 */         } else {\n",
      "/* 064 */           filter_mutableStateArray_0[0].write(0, inputadapter_value_0);\n",
      "/* 065 */         }\n",
      "/* 066 */         append((filter_mutableStateArray_0[0].getRow()));\n",
      "/* 067 */\n",
      "/* 068 */       } while(false);\n",
      "/* 069 */       if (shouldStop()) return;\n",
      "/* 070 */     }\n",
      "/* 071 */   }\n",
      "/* 072 */\n",
      "/* 073 */ }\n",
      "\n",
      "25/01/28 14:40:26 DEBUG CodeGenerator: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 011 */\n",
      "/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 013 */     this.references = references;\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 017 */     partitionIndex = index;\n",
      "/* 018 */     this.inputs = inputs;\n",
      "/* 019 */     inputadapter_input_0 = inputs[0];\n",
      "/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 021 */\n",
      "/* 022 */   }\n",
      "/* 023 */\n",
      "/* 024 */   protected void processNext() throws java.io.IOException {\n",
      "/* 025 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 027 */\n",
      "/* 028 */       do {\n",
      "/* 029 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 030 */         UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 031 */         null : (inputadapter_row_0.getUTF8String(0));\n",
      "/* 032 */\n",
      "/* 033 */         boolean filter_isNull_0 = true;\n",
      "/* 034 */         boolean filter_value_0 = false;\n",
      "/* 035 */         boolean filter_isNull_2 = false;\n",
      "/* 036 */         UTF8String filter_value_2 = null;\n",
      "/* 037 */         if (inputadapter_isNull_0) {\n",
      "/* 038 */           filter_isNull_2 = true;\n",
      "/* 039 */         } else {\n",
      "/* 040 */           filter_value_2 = inputadapter_value_0.trim();\n",
      "/* 041 */         }\n",
      "/* 042 */         boolean filter_isNull_1 = filter_isNull_2;\n",
      "/* 043 */         int filter_value_1 = -1;\n",
      "/* 044 */\n",
      "/* 045 */         if (!filter_isNull_2) {\n",
      "/* 046 */           filter_value_1 = (filter_value_2).numChars();\n",
      "/* 047 */         }\n",
      "/* 048 */         if (!filter_isNull_1) {\n",
      "/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.\n",
      "/* 050 */           filter_value_0 = filter_value_1 > 0;\n",
      "/* 051 */\n",
      "/* 052 */         }\n",
      "/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;\n",
      "/* 054 */\n",
      "/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 056 */\n",
      "/* 057 */         filter_mutableStateArray_0[0].reset();\n",
      "/* 058 */\n",
      "/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 060 */\n",
      "/* 061 */         if (inputadapter_isNull_0) {\n",
      "/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 063 */         } else {\n",
      "/* 064 */           filter_mutableStateArray_0[0].write(0, inputadapter_value_0);\n",
      "/* 065 */         }\n",
      "/* 066 */         append((filter_mutableStateArray_0[0].getRow()));\n",
      "/* 067 */\n",
      "/* 068 */       } while(false);\n",
      "/* 069 */       if (shouldStop()) return;\n",
      "/* 070 */     }\n",
      "/* 071 */   }\n",
      "/* 072 */\n",
      "/* 073 */ }\n",
      "\n",
      "25/01/28 14:40:26 INFO CodeGenerator: Code generated in 70.930958 ms\n",
      "25/01/28 14:40:26 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 200.5 KiB, free 1048.6 MiB)\n",
      "25/01/28 14:40:26 DEBUG BlockManager: Put block broadcast_0 locally took 13 ms\n",
      "25/01/28 14:40:26 DEBUG BlockManager: Putting block broadcast_0 without replication took 13 ms\n",
      "25/01/28 14:40:27 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1048.6 MiB)\n",
      "25/01/28 14:40:27 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_0_piece0 for BlockManagerId(driver, client-172-18-120-79.eduroam.universite-paris-saclay.fr, 50917, None)\n",
      "25/01/28 14:40:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on client-172-18-120-79.eduroam.universite-paris-saclay.fr:50917 (size: 34.7 KiB, free: 1048.8 MiB)\n",
      "25/01/28 14:40:27 DEBUG BlockManagerMaster: Updated info of block broadcast_0_piece0\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Told master about block broadcast_0_piece0\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Put block broadcast_0_piece0 locally took 3 ms\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Putting block broadcast_0_piece0 without replication took 3 ms\n",
      "25/01/28 14:40:27 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0\n",
      "25/01/28 14:40:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/01/28 14:40:27 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "25/01/28 14:40:27 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "25/01/28 14:40:27 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$executeTake$2\n",
      "25/01/28 14:40:27 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++\n",
      "25/01/28 14:40:27 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "25/01/28 14:40:27 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "25/01/28 14:40:27 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "25/01/28 14:40:27 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 3 took 0.000353 seconds\n",
      "25/01/28 14:40:27 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/01/28 14:40:27 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/01/28 14:40:27 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)\n",
      "25/01/28 14:40:27 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/01/28 14:40:27 INFO DAGScheduler: Missing parents: List()\n",
      "25/01/28 14:40:27 DEBUG DAGScheduler: submitStage(ResultStage 0 (name=csv at NativeMethodAccessorImpl.java:0;jobs=0))\n",
      "25/01/28 14:40:27 DEBUG DAGScheduler: missing: List()\n",
      "25/01/28 14:40:27 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/01/28 14:40:27 DEBUG DAGScheduler: submitMissingTasks(ResultStage 0)\n",
      "25/01/28 14:40:27 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 13.5 KiB, free 1048.6 MiB)\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Put block broadcast_1 locally took 0 ms\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Putting block broadcast_1 without replication took 0 ms\n",
      "25/01/28 14:40:27 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 1048.6 MiB)\n",
      "25/01/28 14:40:27 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_1_piece0 for BlockManagerId(driver, client-172-18-120-79.eduroam.universite-paris-saclay.fr, 50917, None)\n",
      "25/01/28 14:40:27 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on client-172-18-120-79.eduroam.universite-paris-saclay.fr:50917 (size: 6.4 KiB, free: 1048.8 MiB)\n",
      "25/01/28 14:40:27 DEBUG BlockManagerMaster: Updated info of block broadcast_1_piece0\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Told master about block broadcast_1_piece0\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Put block broadcast_1_piece0 locally took 0 ms\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Putting block broadcast_1_piece0 without replication took 0 ms\n",
      "25/01/28 14:40:27 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n",
      "25/01/28 14:40:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/01/28 14:40:27 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "25/01/28 14:40:27 DEBUG TaskSetManager: Epoch for TaskSet 0.0: 0\n",
      "25/01/28 14:40:27 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/01/28 14:40:27 DEBUG TaskSetManager: Valid locality levels for TaskSet 0.0: NO_PREF, ANY\n",
      "25/01/28 14:40:27 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_0.0, runningTasks: 0\n",
      "25/01/28 14:40:27 DEBUG TaskSetManager: Valid locality levels for TaskSet 0.0: NO_PREF, ANY\n",
      "25/01/28 14:40:27 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (client-172-18-120-79.eduroam.universite-paris-saclay.fr, executor driver, partition 0, PROCESS_LOCAL, 9682 bytes) \n",
      "25/01/28 14:40:27 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "25/01/28 14:40:27 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "25/01/28 14:40:27 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 1\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Getting local block broadcast_1\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Level for block broadcast_1 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/01/28 14:40:27 DEBUG CodeGenerator: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 011 */\n",
      "/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 013 */     this.references = references;\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 017 */     partitionIndex = index;\n",
      "/* 018 */     this.inputs = inputs;\n",
      "/* 019 */     inputadapter_input_0 = inputs[0];\n",
      "/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 021 */\n",
      "/* 022 */   }\n",
      "/* 023 */\n",
      "/* 024 */   protected void processNext() throws java.io.IOException {\n",
      "/* 025 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 027 */\n",
      "/* 028 */       do {\n",
      "/* 029 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 030 */         UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 031 */         null : (inputadapter_row_0.getUTF8String(0));\n",
      "/* 032 */\n",
      "/* 033 */         boolean filter_isNull_0 = true;\n",
      "/* 034 */         boolean filter_value_0 = false;\n",
      "/* 035 */         boolean filter_isNull_2 = false;\n",
      "/* 036 */         UTF8String filter_value_2 = null;\n",
      "/* 037 */         if (inputadapter_isNull_0) {\n",
      "/* 038 */           filter_isNull_2 = true;\n",
      "/* 039 */         } else {\n",
      "/* 040 */           filter_value_2 = inputadapter_value_0.trim();\n",
      "/* 041 */         }\n",
      "/* 042 */         boolean filter_isNull_1 = filter_isNull_2;\n",
      "/* 043 */         int filter_value_1 = -1;\n",
      "/* 044 */\n",
      "/* 045 */         if (!filter_isNull_2) {\n",
      "/* 046 */           filter_value_1 = (filter_value_2).numChars();\n",
      "/* 047 */         }\n",
      "/* 048 */         if (!filter_isNull_1) {\n",
      "/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.\n",
      "/* 050 */           filter_value_0 = filter_value_1 > 0;\n",
      "/* 051 */\n",
      "/* 052 */         }\n",
      "/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;\n",
      "/* 054 */\n",
      "/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);\n",
      "/* 056 */\n",
      "/* 057 */         filter_mutableStateArray_0[0].reset();\n",
      "/* 058 */\n",
      "/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 060 */\n",
      "/* 061 */         if (inputadapter_isNull_0) {\n",
      "/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);\n",
      "/* 063 */         } else {\n",
      "/* 064 */           filter_mutableStateArray_0[0].write(0, inputadapter_value_0);\n",
      "/* 065 */         }\n",
      "/* 066 */         append((filter_mutableStateArray_0[0].getRow()));\n",
      "/* 067 */\n",
      "/* 068 */       } while(false);\n",
      "/* 069 */       if (shouldStop()) return;\n",
      "/* 070 */     }\n",
      "/* 071 */   }\n",
      "/* 072 */\n",
      "/* 073 */ }\n",
      "\n",
      "25/01/28 14:40:27 INFO CodeGenerator: Code generated in 6.3355 ms\n",
      "25/01/28 14:40:27 INFO FileScanRDD: Reading File path: file:///Users/kempslysilencieux/Documents/bigdata-tuto/spark-code/Spark-ML/Spark-collaborative-filtering/data/movies.csv, range: 0-484687, partition values: [empty row]\n",
      "25/01/28 14:40:27 DEBUG GenerateUnsafeProjection: code for input[0, string, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     UTF8String value_0 = isNull_0 ?\n",
      "/* 033 */     null : (i.getUTF8String(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/01/28 14:40:27 DEBUG CodeGenerator: \n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     UTF8String value_0 = isNull_0 ?\n",
      "/* 033 */     null : (i.getUTF8String(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/01/28 14:40:27 INFO CodeGenerator: Code generated in 5.007291 ms\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Getting local block broadcast_0\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Level for block broadcast_0 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/01/28 14:40:27 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1636 bytes result sent to driver\n",
      "25/01/28 14:40:27 DEBUG ExecutorMetricsPoller: stageTCMP: (0, 0) -> 0\n",
      "25/01/28 14:40:27 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 86 ms on client-172-18-120-79.eduroam.universite-paris-saclay.fr (executor driver) (1/1)\n",
      "25/01/28 14:40:27 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "25/01/28 14:40:27 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 0.131 s\n",
      "25/01/28 14:40:27 DEBUG DAGScheduler: After removal of stage 0, remaining stages = 0\n",
      "25/01/28 14:40:27 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/01/28 14:40:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "25/01/28 14:40:27 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 0.146081 s\n",
      "25/01/28 14:40:27 DEBUG GenerateSafeProjection: code for input[0, string, true].toString:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 024 */     UTF8String value_1 = isNull_1 ?\n",
      "/* 025 */     null : (i.getUTF8String(0));\n",
      "/* 026 */     boolean isNull_0 = true;\n",
      "/* 027 */     java.lang.String value_0 = null;\n",
      "/* 028 */     if (!isNull_1) {\n",
      "/* 029 */       isNull_0 = false;\n",
      "/* 030 */       if (!isNull_0) {\n",
      "/* 031 */\n",
      "/* 032 */         Object funcResult_0 = null;\n",
      "/* 033 */         funcResult_0 = value_1.toString();\n",
      "/* 034 */         value_0 = (java.lang.String) funcResult_0;\n",
      "/* 035 */\n",
      "/* 036 */       }\n",
      "/* 037 */     }\n",
      "/* 038 */     if (isNull_0) {\n",
      "/* 039 */       mutableRow.setNullAt(0);\n",
      "/* 040 */     } else {\n",
      "/* 041 */\n",
      "/* 042 */       mutableRow.update(0, value_0);\n",
      "/* 043 */     }\n",
      "/* 044 */\n",
      "/* 045 */     return mutableRow;\n",
      "/* 046 */   }\n",
      "/* 047 */\n",
      "/* 048 */\n",
      "/* 049 */ }\n",
      "\n",
      "25/01/28 14:40:27 DEBUG CodeGenerator: \n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 024 */     UTF8String value_1 = isNull_1 ?\n",
      "/* 025 */     null : (i.getUTF8String(0));\n",
      "/* 026 */     boolean isNull_0 = true;\n",
      "/* 027 */     java.lang.String value_0 = null;\n",
      "/* 028 */     if (!isNull_1) {\n",
      "/* 029 */       isNull_0 = false;\n",
      "/* 030 */       if (!isNull_0) {\n",
      "/* 031 */\n",
      "/* 032 */         Object funcResult_0 = null;\n",
      "/* 033 */         funcResult_0 = value_1.toString();\n",
      "/* 034 */         value_0 = (java.lang.String) funcResult_0;\n",
      "/* 035 */\n",
      "/* 036 */       }\n",
      "/* 037 */     }\n",
      "/* 038 */     if (isNull_0) {\n",
      "/* 039 */       mutableRow.setNullAt(0);\n",
      "/* 040 */     } else {\n",
      "/* 041 */\n",
      "/* 042 */       mutableRow.update(0, value_0);\n",
      "/* 043 */     }\n",
      "/* 044 */\n",
      "/* 045 */     return mutableRow;\n",
      "/* 046 */   }\n",
      "/* 047 */\n",
      "/* 048 */\n",
      "/* 049 */ }\n",
      "\n",
      "25/01/28 14:40:27 INFO CodeGenerator: Code generated in 3.877375 ms\n",
      "25/01/28 14:40:27 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/01/28 14:40:27 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/01/28 14:40:27 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 200.5 KiB, free 1048.4 MiB)\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Put block broadcast_2 locally took 2 ms\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Putting block broadcast_2 without replication took 2 ms\n",
      "25/01/28 14:40:27 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1048.3 MiB)\n",
      "25/01/28 14:40:27 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_2_piece0 for BlockManagerId(driver, client-172-18-120-79.eduroam.universite-paris-saclay.fr, 50917, None)\n",
      "25/01/28 14:40:27 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on client-172-18-120-79.eduroam.universite-paris-saclay.fr:50917 (size: 34.7 KiB, free: 1048.7 MiB)\n",
      "25/01/28 14:40:27 DEBUG BlockManagerMaster: Updated info of block broadcast_2_piece0\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Told master about block broadcast_2_piece0\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Put block broadcast_2_piece0 locally took 0 ms\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Putting block broadcast_2_piece0 without replication took 0 ms\n",
      "25/01/28 14:40:27 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0\n",
      "25/01/28 14:40:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/01/28 14:40:27 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$rdd$1\n",
      "25/01/28 14:40:27 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$rdd$1) is now cleaned +++\n",
      "25/01/28 14:40:27 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$inferFromDataset$2\n",
      "25/01/28 14:40:27 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$inferFromDataset$2) is now cleaned +++\n",
      "25/01/28 14:40:27 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$infer$2\n",
      "25/01/28 14:40:27 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$infer$2) is now cleaned +++\n",
      "25/01/28 14:40:27 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$infer$3\n",
      "25/01/28 14:40:27 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$infer$3) is now cleaned +++\n",
      "25/01/28 14:40:27 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$6\n",
      "25/01/28 14:40:27 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$6) is now cleaned +++\n",
      "25/01/28 14:40:27 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n",
      "25/01/28 14:40:27 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 9 took 0.000067 seconds\n",
      "25/01/28 14:40:27 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/01/28 14:40:27 INFO DAGScheduler: Got job 1 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/01/28 14:40:27 INFO DAGScheduler: Final stage: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0)\n",
      "25/01/28 14:40:27 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/01/28 14:40:27 INFO DAGScheduler: Missing parents: List()\n",
      "25/01/28 14:40:27 DEBUG DAGScheduler: submitStage(ResultStage 1 (name=csv at NativeMethodAccessorImpl.java:0;jobs=1))\n",
      "25/01/28 14:40:27 DEBUG DAGScheduler: missing: List()\n",
      "25/01/28 14:40:27 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/01/28 14:40:27 DEBUG DAGScheduler: submitMissingTasks(ResultStage 1)\n",
      "25/01/28 14:40:27 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 27.9 KiB, free 1048.3 MiB)\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Put block broadcast_3 locally took 0 ms\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Putting block broadcast_3 without replication took 0 ms\n",
      "25/01/28 14:40:27 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 12.9 KiB, free 1048.3 MiB)\n",
      "25/01/28 14:40:27 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_3_piece0 for BlockManagerId(driver, client-172-18-120-79.eduroam.universite-paris-saclay.fr, 50917, None)\n",
      "25/01/28 14:40:27 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on client-172-18-120-79.eduroam.universite-paris-saclay.fr:50917 (size: 12.9 KiB, free: 1048.7 MiB)\n",
      "25/01/28 14:40:27 DEBUG BlockManagerMaster: Updated info of block broadcast_3_piece0\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Told master about block broadcast_3_piece0\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Put block broadcast_3_piece0 locally took 0 ms\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Putting block broadcast_3_piece0 without replication took 0 ms\n",
      "25/01/28 14:40:27 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585\n",
      "25/01/28 14:40:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/01/28 14:40:27 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "25/01/28 14:40:27 DEBUG TaskSetManager: Epoch for TaskSet 1.0: 0\n",
      "25/01/28 14:40:27 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/01/28 14:40:27 DEBUG TaskSetManager: Valid locality levels for TaskSet 1.0: NO_PREF, ANY\n",
      "25/01/28 14:40:27 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_1.0, runningTasks: 0\n",
      "25/01/28 14:40:27 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (client-172-18-120-79.eduroam.universite-paris-saclay.fr, executor driver, partition 0, PROCESS_LOCAL, 9682 bytes) \n",
      "25/01/28 14:40:27 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "25/01/28 14:40:27 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "25/01/28 14:40:27 DEBUG ExecutorMetricsPoller: stageTCMP: (1, 0) -> 1\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Getting local block broadcast_3\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Level for block broadcast_3 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/01/28 14:40:27 DEBUG GenerateSafeProjection: code for input[0, string, true].toString:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 024 */     UTF8String value_1 = isNull_1 ?\n",
      "/* 025 */     null : (i.getUTF8String(0));\n",
      "/* 026 */     boolean isNull_0 = true;\n",
      "/* 027 */     java.lang.String value_0 = null;\n",
      "/* 028 */     if (!isNull_1) {\n",
      "/* 029 */       isNull_0 = false;\n",
      "/* 030 */       if (!isNull_0) {\n",
      "/* 031 */\n",
      "/* 032 */         Object funcResult_0 = null;\n",
      "/* 033 */         funcResult_0 = value_1.toString();\n",
      "/* 034 */         value_0 = (java.lang.String) funcResult_0;\n",
      "/* 035 */\n",
      "/* 036 */       }\n",
      "/* 037 */     }\n",
      "/* 038 */     if (isNull_0) {\n",
      "/* 039 */       mutableRow.setNullAt(0);\n",
      "/* 040 */     } else {\n",
      "/* 041 */\n",
      "/* 042 */       mutableRow.update(0, value_0);\n",
      "/* 043 */     }\n",
      "/* 044 */\n",
      "/* 045 */     return mutableRow;\n",
      "/* 046 */   }\n",
      "/* 047 */\n",
      "/* 048 */\n",
      "/* 049 */ }\n",
      "\n",
      "25/01/28 14:40:27 DEBUG CodeGenerator: \n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     boolean isNull_1 = i.isNullAt(0);\n",
      "/* 024 */     UTF8String value_1 = isNull_1 ?\n",
      "/* 025 */     null : (i.getUTF8String(0));\n",
      "/* 026 */     boolean isNull_0 = true;\n",
      "/* 027 */     java.lang.String value_0 = null;\n",
      "/* 028 */     if (!isNull_1) {\n",
      "/* 029 */       isNull_0 = false;\n",
      "/* 030 */       if (!isNull_0) {\n",
      "/* 031 */\n",
      "/* 032 */         Object funcResult_0 = null;\n",
      "/* 033 */         funcResult_0 = value_1.toString();\n",
      "/* 034 */         value_0 = (java.lang.String) funcResult_0;\n",
      "/* 035 */\n",
      "/* 036 */       }\n",
      "/* 037 */     }\n",
      "/* 038 */     if (isNull_0) {\n",
      "/* 039 */       mutableRow.setNullAt(0);\n",
      "/* 040 */     } else {\n",
      "/* 041 */\n",
      "/* 042 */       mutableRow.update(0, value_0);\n",
      "/* 043 */     }\n",
      "/* 044 */\n",
      "/* 045 */     return mutableRow;\n",
      "/* 046 */   }\n",
      "/* 047 */\n",
      "/* 048 */\n",
      "/* 049 */ }\n",
      "\n",
      "25/01/28 14:40:27 INFO CodeGenerator: Code generated in 3.492125 ms\n",
      "25/01/28 14:40:27 INFO FileScanRDD: Reading File path: file:///Users/kempslysilencieux/Documents/bigdata-tuto/spark-code/Spark-ML/Spark-collaborative-filtering/data/movies.csv, range: 0-484687, partition values: [empty row]\n",
      "25/01/28 14:40:27 DEBUG GenerateUnsafeProjection: code for input[0, string, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     UTF8String value_0 = isNull_0 ?\n",
      "/* 033 */     null : (i.getUTF8String(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 040 */   }\n",
      "/* 041 */\n",
      "/* 042 */\n",
      "/* 043 */ }\n",
      "\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Getting local block broadcast_2\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Level for block broadcast_2 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/01/28 14:40:27 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 1573 bytes result sent to driver\n",
      "25/01/28 14:40:27 DEBUG ExecutorMetricsPoller: stageTCMP: (1, 0) -> 0\n",
      "25/01/28 14:40:27 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 67 ms on client-172-18-120-79.eduroam.universite-paris-saclay.fr (executor driver) (1/1)\n",
      "25/01/28 14:40:27 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "25/01/28 14:40:27 INFO DAGScheduler: ResultStage 1 (csv at NativeMethodAccessorImpl.java:0) finished in 0.082 s\n",
      "25/01/28 14:40:27 DEBUG DAGScheduler: After removal of stage 1, remaining stages = 0\n",
      "25/01/28 14:40:27 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/01/28 14:40:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "25/01/28 14:40:27 INFO DAGScheduler: Job 1 finished: csv at NativeMethodAccessorImpl.java:0, took 0.084783 s\n",
      "25/01/28 14:40:27 INFO FileSourceStrategy: Pushed Filters: \n",
      "25/01/28 14:40:27 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "25/01/28 14:40:27 DEBUG WholeStageCodegenExec: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 011 */\n",
      "/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 013 */     this.references = references;\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 017 */     partitionIndex = index;\n",
      "/* 018 */     this.inputs = inputs;\n",
      "/* 019 */     inputadapter_input_0 = inputs[0];\n",
      "/* 020 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 96);\n",
      "/* 021 */\n",
      "/* 022 */   }\n",
      "/* 023 */\n",
      "/* 024 */   protected void processNext() throws java.io.IOException {\n",
      "/* 025 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 027 */\n",
      "/* 028 */       // common sub-expressions\n",
      "/* 029 */\n",
      "/* 030 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 031 */       int inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 032 */       -1 : (inputadapter_row_0.getInt(0));\n",
      "/* 033 */       UTF8String project_value_0;\n",
      "/* 034 */       if (inputadapter_isNull_0) {\n",
      "/* 035 */         project_value_0 = UTF8String.fromString(\"NULL\");\n",
      "/* 036 */       } else {\n",
      "/* 037 */         project_value_0 = UTF8String.fromString(String.valueOf(inputadapter_value_0));\n",
      "/* 038 */       }\n",
      "/* 039 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 040 */       UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 041 */       null : (inputadapter_row_0.getUTF8String(1));\n",
      "/* 042 */       UTF8String project_value_2;\n",
      "/* 043 */       if (inputadapter_isNull_1) {\n",
      "/* 044 */         project_value_2 = UTF8String.fromString(\"NULL\");\n",
      "/* 045 */       } else {\n",
      "/* 046 */         project_value_2 = inputadapter_value_1;\n",
      "/* 047 */       }\n",
      "/* 048 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);\n",
      "/* 049 */       UTF8String inputadapter_value_2 = inputadapter_isNull_2 ?\n",
      "/* 050 */       null : (inputadapter_row_0.getUTF8String(2));\n",
      "/* 051 */       UTF8String project_value_4;\n",
      "/* 052 */       if (inputadapter_isNull_2) {\n",
      "/* 053 */         project_value_4 = UTF8String.fromString(\"NULL\");\n",
      "/* 054 */       } else {\n",
      "/* 055 */         project_value_4 = inputadapter_value_2;\n",
      "/* 056 */       }\n",
      "/* 057 */       project_mutableStateArray_0[0].reset();\n",
      "/* 058 */\n",
      "/* 059 */       project_mutableStateArray_0[0].write(0, project_value_0);\n",
      "/* 060 */\n",
      "/* 061 */       project_mutableStateArray_0[0].write(1, project_value_2);\n",
      "/* 062 */\n",
      "/* 063 */       project_mutableStateArray_0[0].write(2, project_value_4);\n",
      "/* 064 */       append((project_mutableStateArray_0[0].getRow()));\n",
      "/* 065 */       if (shouldStop()) return;\n",
      "/* 066 */     }\n",
      "/* 067 */   }\n",
      "/* 068 */\n",
      "/* 069 */ }\n",
      "\n",
      "25/01/28 14:40:27 DEBUG CodeGenerator: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 011 */\n",
      "/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 013 */     this.references = references;\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 017 */     partitionIndex = index;\n",
      "/* 018 */     this.inputs = inputs;\n",
      "/* 019 */     inputadapter_input_0 = inputs[0];\n",
      "/* 020 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 96);\n",
      "/* 021 */\n",
      "/* 022 */   }\n",
      "/* 023 */\n",
      "/* 024 */   protected void processNext() throws java.io.IOException {\n",
      "/* 025 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 027 */\n",
      "/* 028 */       // common sub-expressions\n",
      "/* 029 */\n",
      "/* 030 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 031 */       int inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 032 */       -1 : (inputadapter_row_0.getInt(0));\n",
      "/* 033 */       UTF8String project_value_0;\n",
      "/* 034 */       if (inputadapter_isNull_0) {\n",
      "/* 035 */         project_value_0 = UTF8String.fromString(\"NULL\");\n",
      "/* 036 */       } else {\n",
      "/* 037 */         project_value_0 = UTF8String.fromString(String.valueOf(inputadapter_value_0));\n",
      "/* 038 */       }\n",
      "/* 039 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 040 */       UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 041 */       null : (inputadapter_row_0.getUTF8String(1));\n",
      "/* 042 */       UTF8String project_value_2;\n",
      "/* 043 */       if (inputadapter_isNull_1) {\n",
      "/* 044 */         project_value_2 = UTF8String.fromString(\"NULL\");\n",
      "/* 045 */       } else {\n",
      "/* 046 */         project_value_2 = inputadapter_value_1;\n",
      "/* 047 */       }\n",
      "/* 048 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);\n",
      "/* 049 */       UTF8String inputadapter_value_2 = inputadapter_isNull_2 ?\n",
      "/* 050 */       null : (inputadapter_row_0.getUTF8String(2));\n",
      "/* 051 */       UTF8String project_value_4;\n",
      "/* 052 */       if (inputadapter_isNull_2) {\n",
      "/* 053 */         project_value_4 = UTF8String.fromString(\"NULL\");\n",
      "/* 054 */       } else {\n",
      "/* 055 */         project_value_4 = inputadapter_value_2;\n",
      "/* 056 */       }\n",
      "/* 057 */       project_mutableStateArray_0[0].reset();\n",
      "/* 058 */\n",
      "/* 059 */       project_mutableStateArray_0[0].write(0, project_value_0);\n",
      "/* 060 */\n",
      "/* 061 */       project_mutableStateArray_0[0].write(1, project_value_2);\n",
      "/* 062 */\n",
      "/* 063 */       project_mutableStateArray_0[0].write(2, project_value_4);\n",
      "/* 064 */       append((project_mutableStateArray_0[0].getRow()));\n",
      "/* 065 */       if (shouldStop()) return;\n",
      "/* 066 */     }\n",
      "/* 067 */   }\n",
      "/* 068 */\n",
      "/* 069 */ }\n",
      "\n",
      "25/01/28 14:40:27 INFO CodeGenerator: Code generated in 5.75925 ms\n",
      "25/01/28 14:40:27 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 200.4 KiB, free 1048.1 MiB)\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Put block broadcast_4 locally took 1 ms\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Putting block broadcast_4 without replication took 1 ms\n",
      "25/01/28 14:40:27 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.7 KiB, free 1048.1 MiB)\n",
      "25/01/28 14:40:27 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_4_piece0 for BlockManagerId(driver, client-172-18-120-79.eduroam.universite-paris-saclay.fr, 50917, None)\n",
      "25/01/28 14:40:27 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on client-172-18-120-79.eduroam.universite-paris-saclay.fr:50917 (size: 34.7 KiB, free: 1048.7 MiB)\n",
      "25/01/28 14:40:27 DEBUG BlockManagerMaster: Updated info of block broadcast_4_piece0\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Told master about block broadcast_4_piece0\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Put block broadcast_4_piece0 locally took 0 ms\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Putting block broadcast_4_piece0 without replication took 0 ms\n",
      "25/01/28 14:40:27 INFO SparkContext: Created broadcast 4 from showString at NativeMethodAccessorImpl.java:0\n",
      "25/01/28 14:40:27 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "25/01/28 14:40:27 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$doExecute$4$adapted\n",
      "25/01/28 14:40:27 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++\n",
      "25/01/28 14:40:27 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$executeTake$2\n",
      "25/01/28 14:40:27 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++\n",
      "25/01/28 14:40:27 DEBUG ClosureCleaner: Cleaning indylambda closure: $anonfun$runJob$5\n",
      "25/01/28 14:40:27 DEBUG ClosureCleaner:  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++\n",
      "25/01/28 14:40:27 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "25/01/28 14:40:27 DEBUG DAGScheduler: eagerlyComputePartitionsForRddAndAncestors for RDD 13 took 0.000048 seconds\n",
      "25/01/28 14:40:27 DEBUG DAGScheduler: Merging stage rdd profiles: Set()\n",
      "25/01/28 14:40:27 INFO DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "25/01/28 14:40:27 INFO DAGScheduler: Final stage: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0)\n",
      "25/01/28 14:40:27 INFO DAGScheduler: Parents of final stage: List()\n",
      "25/01/28 14:40:27 INFO DAGScheduler: Missing parents: List()\n",
      "25/01/28 14:40:27 DEBUG DAGScheduler: submitStage(ResultStage 2 (name=showString at NativeMethodAccessorImpl.java:0;jobs=2))\n",
      "25/01/28 14:40:27 DEBUG DAGScheduler: missing: List()\n",
      "25/01/28 14:40:27 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "25/01/28 14:40:27 DEBUG DAGScheduler: submitMissingTasks(ResultStage 2)\n",
      "25/01/28 14:40:27 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 15.8 KiB, free 1048.0 MiB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies Dataset:\n",
      "+-------+--------------------+--------------------+\n",
      "|movieId|               title|              genres|\n",
      "+-------+--------------------+--------------------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|\n",
      "|      2|      Jumanji (1995)|Adventure|Childre...|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|\n",
      "|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n",
      "|      5|Father of the Bri...|              Comedy|\n",
      "+-------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Ratings Dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/28 14:40:27 DEBUG BlockManager: Put block broadcast_5 locally took 0 ms\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Putting block broadcast_5 without replication took 0 ms\n",
      "25/01/28 14:40:27 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 7.6 KiB, free 1048.0 MiB)\n",
      "25/01/28 14:40:27 DEBUG BlockManagerMasterEndpoint: Updating block info on master broadcast_5_piece0 for BlockManagerId(driver, client-172-18-120-79.eduroam.universite-paris-saclay.fr, 50917, None)\n",
      "25/01/28 14:40:27 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on client-172-18-120-79.eduroam.universite-paris-saclay.fr:50917 (size: 7.6 KiB, free: 1048.7 MiB)\n",
      "25/01/28 14:40:27 DEBUG BlockManagerMaster: Updated info of block broadcast_5_piece0\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Told master about block broadcast_5_piece0\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Put block broadcast_5_piece0 locally took 0 ms\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Putting block broadcast_5_piece0 without replication took 0 ms\n",
      "25/01/28 14:40:27 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585\n",
      "25/01/28 14:40:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "25/01/28 14:40:27 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0\n",
      "25/01/28 14:40:27 DEBUG TaskSetManager: Epoch for TaskSet 2.0: 0\n",
      "25/01/28 14:40:27 DEBUG TaskSetManager: Adding pending tasks took 0 ms\n",
      "25/01/28 14:40:27 DEBUG TaskSetManager: Valid locality levels for TaskSet 2.0: NO_PREF, ANY\n",
      "25/01/28 14:40:27 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_2.0, runningTasks: 0\n",
      "25/01/28 14:40:27 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (client-172-18-120-79.eduroam.universite-paris-saclay.fr, executor driver, partition 0, PROCESS_LOCAL, 9682 bytes) \n",
      "25/01/28 14:40:27 DEBUG TaskSetManager: No tasks for locality level NO_PREF, so moving to locality level ANY\n",
      "25/01/28 14:40:27 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)\n",
      "25/01/28 14:40:27 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 1\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Getting local block broadcast_5\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Level for block broadcast_5 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/01/28 14:40:27 DEBUG CodeGenerator: \n",
      "/* 001 */ public Object generate(Object[] references) {\n",
      "/* 002 */   return new GeneratedIteratorForCodegenStage1(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ // codegenStageId=1\n",
      "/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private scala.collection.Iterator[] inputs;\n",
      "/* 009 */   private scala.collection.Iterator inputadapter_input_0;\n",
      "/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 011 */\n",
      "/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {\n",
      "/* 013 */     this.references = references;\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {\n",
      "/* 017 */     partitionIndex = index;\n",
      "/* 018 */     this.inputs = inputs;\n",
      "/* 019 */     inputadapter_input_0 = inputs[0];\n",
      "/* 020 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 96);\n",
      "/* 021 */\n",
      "/* 022 */   }\n",
      "/* 023 */\n",
      "/* 024 */   protected void processNext() throws java.io.IOException {\n",
      "/* 025 */     while ( inputadapter_input_0.hasNext()) {\n",
      "/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();\n",
      "/* 027 */\n",
      "/* 028 */       // common sub-expressions\n",
      "/* 029 */\n",
      "/* 030 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);\n",
      "/* 031 */       int inputadapter_value_0 = inputadapter_isNull_0 ?\n",
      "/* 032 */       -1 : (inputadapter_row_0.getInt(0));\n",
      "/* 033 */       UTF8String project_value_0;\n",
      "/* 034 */       if (inputadapter_isNull_0) {\n",
      "/* 035 */         project_value_0 = UTF8String.fromString(\"NULL\");\n",
      "/* 036 */       } else {\n",
      "/* 037 */         project_value_0 = UTF8String.fromString(String.valueOf(inputadapter_value_0));\n",
      "/* 038 */       }\n",
      "/* 039 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);\n",
      "/* 040 */       UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?\n",
      "/* 041 */       null : (inputadapter_row_0.getUTF8String(1));\n",
      "/* 042 */       UTF8String project_value_2;\n",
      "/* 043 */       if (inputadapter_isNull_1) {\n",
      "/* 044 */         project_value_2 = UTF8String.fromString(\"NULL\");\n",
      "/* 045 */       } else {\n",
      "/* 046 */         project_value_2 = inputadapter_value_1;\n",
      "/* 047 */       }\n",
      "/* 048 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);\n",
      "/* 049 */       UTF8String inputadapter_value_2 = inputadapter_isNull_2 ?\n",
      "/* 050 */       null : (inputadapter_row_0.getUTF8String(2));\n",
      "/* 051 */       UTF8String project_value_4;\n",
      "/* 052 */       if (inputadapter_isNull_2) {\n",
      "/* 053 */         project_value_4 = UTF8String.fromString(\"NULL\");\n",
      "/* 054 */       } else {\n",
      "/* 055 */         project_value_4 = inputadapter_value_2;\n",
      "/* 056 */       }\n",
      "/* 057 */       project_mutableStateArray_0[0].reset();\n",
      "/* 058 */\n",
      "/* 059 */       project_mutableStateArray_0[0].write(0, project_value_0);\n",
      "/* 060 */\n",
      "/* 061 */       project_mutableStateArray_0[0].write(1, project_value_2);\n",
      "/* 062 */\n",
      "/* 063 */       project_mutableStateArray_0[0].write(2, project_value_4);\n",
      "/* 064 */       append((project_mutableStateArray_0[0].getRow()));\n",
      "/* 065 */       if (shouldStop()) return;\n",
      "/* 066 */     }\n",
      "/* 067 */   }\n",
      "/* 068 */\n",
      "/* 069 */ }\n",
      "\n",
      "25/01/28 14:40:27 INFO CodeGenerator: Code generated in 4.164875 ms\n",
      "25/01/28 14:40:27 INFO FileScanRDD: Reading File path: file:///Users/kempslysilencieux/Documents/bigdata-tuto/spark-code/Spark-ML/Spark-collaborative-filtering/data/movies.csv, range: 0-484687, partition values: [empty row]\n",
      "25/01/28 14:40:27 DEBUG GenerateUnsafeProjection: code for input[0, int, true],input[1, string, true],input[2, string, true]:\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 64);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     int value_0 = isNull_0 ?\n",
      "/* 033 */     -1 : (i.getInt(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */\n",
      "/* 040 */     boolean isNull_1 = i.isNullAt(1);\n",
      "/* 041 */     UTF8String value_1 = isNull_1 ?\n",
      "/* 042 */     null : (i.getUTF8String(1));\n",
      "/* 043 */     if (isNull_1) {\n",
      "/* 044 */       mutableStateArray_0[0].setNullAt(1);\n",
      "/* 045 */     } else {\n",
      "/* 046 */       mutableStateArray_0[0].write(1, value_1);\n",
      "/* 047 */     }\n",
      "/* 048 */\n",
      "/* 049 */     boolean isNull_2 = i.isNullAt(2);\n",
      "/* 050 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 051 */     null : (i.getUTF8String(2));\n",
      "/* 052 */     if (isNull_2) {\n",
      "/* 053 */       mutableStateArray_0[0].setNullAt(2);\n",
      "/* 054 */     } else {\n",
      "/* 055 */       mutableStateArray_0[0].write(2, value_2);\n",
      "/* 056 */     }\n",
      "/* 057 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 058 */   }\n",
      "/* 059 */\n",
      "/* 060 */\n",
      "/* 061 */ }\n",
      "\n",
      "25/01/28 14:40:27 DEBUG CodeGenerator: \n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificUnsafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];\n",
      "/* 009 */\n",
      "/* 010 */   public SpecificUnsafeProjection(Object[] references) {\n",
      "/* 011 */     this.references = references;\n",
      "/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(3, 64);\n",
      "/* 013 */\n",
      "/* 014 */   }\n",
      "/* 015 */\n",
      "/* 016 */   public void initialize(int partitionIndex) {\n",
      "/* 017 */\n",
      "/* 018 */   }\n",
      "/* 019 */\n",
      "/* 020 */   // Scala.Function1 need this\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object row) {\n",
      "/* 022 */     return apply((InternalRow) row);\n",
      "/* 023 */   }\n",
      "/* 024 */\n",
      "/* 025 */   public UnsafeRow apply(InternalRow i) {\n",
      "/* 026 */     mutableStateArray_0[0].reset();\n",
      "/* 027 */\n",
      "/* 028 */\n",
      "/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();\n",
      "/* 030 */\n",
      "/* 031 */     boolean isNull_0 = i.isNullAt(0);\n",
      "/* 032 */     int value_0 = isNull_0 ?\n",
      "/* 033 */     -1 : (i.getInt(0));\n",
      "/* 034 */     if (isNull_0) {\n",
      "/* 035 */       mutableStateArray_0[0].setNullAt(0);\n",
      "/* 036 */     } else {\n",
      "/* 037 */       mutableStateArray_0[0].write(0, value_0);\n",
      "/* 038 */     }\n",
      "/* 039 */\n",
      "/* 040 */     boolean isNull_1 = i.isNullAt(1);\n",
      "/* 041 */     UTF8String value_1 = isNull_1 ?\n",
      "/* 042 */     null : (i.getUTF8String(1));\n",
      "/* 043 */     if (isNull_1) {\n",
      "/* 044 */       mutableStateArray_0[0].setNullAt(1);\n",
      "/* 045 */     } else {\n",
      "/* 046 */       mutableStateArray_0[0].write(1, value_1);\n",
      "/* 047 */     }\n",
      "/* 048 */\n",
      "/* 049 */     boolean isNull_2 = i.isNullAt(2);\n",
      "/* 050 */     UTF8String value_2 = isNull_2 ?\n",
      "/* 051 */     null : (i.getUTF8String(2));\n",
      "/* 052 */     if (isNull_2) {\n",
      "/* 053 */       mutableStateArray_0[0].setNullAt(2);\n",
      "/* 054 */     } else {\n",
      "/* 055 */       mutableStateArray_0[0].write(2, value_2);\n",
      "/* 056 */     }\n",
      "/* 057 */     return (mutableStateArray_0[0].getRow());\n",
      "/* 058 */   }\n",
      "/* 059 */\n",
      "/* 060 */\n",
      "/* 061 */ }\n",
      "\n",
      "25/01/28 14:40:27 INFO CodeGenerator: Code generated in 7.353709 ms\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Getting local block broadcast_4\n",
      "25/01/28 14:40:27 DEBUG BlockManager: Level for block broadcast_4 is StorageLevel(disk, memory, deserialized, 1 replicas)\n",
      "25/01/28 14:40:27 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 1851 bytes result sent to driver\n",
      "25/01/28 14:40:27 DEBUG ExecutorMetricsPoller: stageTCMP: (2, 0) -> 0\n",
      "25/01/28 14:40:27 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 30 ms on client-172-18-120-79.eduroam.universite-paris-saclay.fr (executor driver) (1/1)\n",
      "25/01/28 14:40:27 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "25/01/28 14:40:27 INFO DAGScheduler: ResultStage 2 (showString at NativeMethodAccessorImpl.java:0) finished in 0.033 s\n",
      "25/01/28 14:40:27 DEBUG DAGScheduler: After removal of stage 2, remaining stages = 0\n",
      "25/01/28 14:40:27 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "25/01/28 14:40:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished\n",
      "25/01/28 14:40:27 INFO DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.035259 s\n",
      "25/01/28 14:40:27 DEBUG GenerateSafeProjection: code for createexternalrow(input[0, string, false].toString, input[1, string, false].toString, input[2, string, false].toString, StructField(toprettystring(movieId),StringType,false), StructField(toprettystring(title),StringType,false), StructField(toprettystring(genres),StringType,false)):\n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     UTF8String value_2 = i.getUTF8String(0);\n",
      "/* 039 */     boolean isNull_1 = true;\n",
      "/* 040 */     java.lang.String value_1 = null;\n",
      "/* 041 */     isNull_1 = false;\n",
      "/* 042 */     if (!isNull_1) {\n",
      "/* 043 */\n",
      "/* 044 */       Object funcResult_0 = null;\n",
      "/* 045 */       funcResult_0 = value_2.toString();\n",
      "/* 046 */       value_1 = (java.lang.String) funcResult_0;\n",
      "/* 047 */\n",
      "/* 048 */     }\n",
      "/* 049 */     if (isNull_1) {\n",
      "/* 050 */       values_0[0] = null;\n",
      "/* 051 */     } else {\n",
      "/* 052 */       values_0[0] = value_1;\n",
      "/* 053 */     }\n",
      "/* 054 */\n",
      "/* 055 */     UTF8String value_4 = i.getUTF8String(1);\n",
      "/* 056 */     boolean isNull_3 = true;\n",
      "/* 057 */     java.lang.String value_3 = null;\n",
      "/* 058 */     isNull_3 = false;\n",
      "/* 059 */     if (!isNull_3) {\n",
      "/* 060 */\n",
      "/* 061 */       Object funcResult_1 = null;\n",
      "/* 062 */       funcResult_1 = value_4.toString();\n",
      "/* 063 */       value_3 = (java.lang.String) funcResult_1;\n",
      "/* 064 */\n",
      "/* 065 */     }\n",
      "/* 066 */     if (isNull_3) {\n",
      "/* 067 */       values_0[1] = null;\n",
      "/* 068 */     } else {\n",
      "/* 069 */       values_0[1] = value_3;\n",
      "/* 070 */     }\n",
      "/* 071 */\n",
      "/* 072 */     UTF8String value_6 = i.getUTF8String(2);\n",
      "/* 073 */     boolean isNull_5 = true;\n",
      "/* 074 */     java.lang.String value_5 = null;\n",
      "/* 075 */     isNull_5 = false;\n",
      "/* 076 */     if (!isNull_5) {\n",
      "/* 077 */\n",
      "/* 078 */       Object funcResult_2 = null;\n",
      "/* 079 */       funcResult_2 = value_6.toString();\n",
      "/* 080 */       value_5 = (java.lang.String) funcResult_2;\n",
      "/* 081 */\n",
      "/* 082 */     }\n",
      "/* 083 */     if (isNull_5) {\n",
      "/* 084 */       values_0[2] = null;\n",
      "/* 085 */     } else {\n",
      "/* 086 */       values_0[2] = value_5;\n",
      "/* 087 */     }\n",
      "/* 088 */\n",
      "/* 089 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 090 */\n",
      "/* 091 */     return value_0;\n",
      "/* 092 */   }\n",
      "/* 093 */\n",
      "/* 094 */ }\n",
      "\n",
      "25/01/28 14:40:27 DEBUG CodeGenerator: \n",
      "/* 001 */ public java.lang.Object generate(Object[] references) {\n",
      "/* 002 */   return new SpecificSafeProjection(references);\n",
      "/* 003 */ }\n",
      "/* 004 */\n",
      "/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {\n",
      "/* 006 */\n",
      "/* 007 */   private Object[] references;\n",
      "/* 008 */   private InternalRow mutableRow;\n",
      "/* 009 */\n",
      "/* 010 */\n",
      "/* 011 */   public SpecificSafeProjection(Object[] references) {\n",
      "/* 012 */     this.references = references;\n",
      "/* 013 */     mutableRow = (InternalRow) references[references.length - 1];\n",
      "/* 014 */\n",
      "/* 015 */   }\n",
      "/* 016 */\n",
      "/* 017 */   public void initialize(int partitionIndex) {\n",
      "/* 018 */\n",
      "/* 019 */   }\n",
      "/* 020 */\n",
      "/* 021 */   public java.lang.Object apply(java.lang.Object _i) {\n",
      "/* 022 */     InternalRow i = (InternalRow) _i;\n",
      "/* 023 */     org.apache.spark.sql.Row value_7 = CreateExternalRow_0(i);\n",
      "/* 024 */     if (false) {\n",
      "/* 025 */       mutableRow.setNullAt(0);\n",
      "/* 026 */     } else {\n",
      "/* 027 */\n",
      "/* 028 */       mutableRow.update(0, value_7);\n",
      "/* 029 */     }\n",
      "/* 030 */\n",
      "/* 031 */     return mutableRow;\n",
      "/* 032 */   }\n",
      "/* 033 */\n",
      "/* 034 */\n",
      "/* 035 */   private org.apache.spark.sql.Row CreateExternalRow_0(InternalRow i) {\n",
      "/* 036 */     Object[] values_0 = new Object[3];\n",
      "/* 037 */\n",
      "/* 038 */     UTF8String value_2 = i.getUTF8String(0);\n",
      "/* 039 */     boolean isNull_1 = true;\n",
      "/* 040 */     java.lang.String value_1 = null;\n",
      "/* 041 */     isNull_1 = false;\n",
      "/* 042 */     if (!isNull_1) {\n",
      "/* 043 */\n",
      "/* 044 */       Object funcResult_0 = null;\n",
      "/* 045 */       funcResult_0 = value_2.toString();\n",
      "/* 046 */       value_1 = (java.lang.String) funcResult_0;\n",
      "/* 047 */\n",
      "/* 048 */     }\n",
      "/* 049 */     if (isNull_1) {\n",
      "/* 050 */       values_0[0] = null;\n",
      "/* 051 */     } else {\n",
      "/* 052 */       values_0[0] = value_1;\n",
      "/* 053 */     }\n",
      "/* 054 */\n",
      "/* 055 */     UTF8String value_4 = i.getUTF8String(1);\n",
      "/* 056 */     boolean isNull_3 = true;\n",
      "/* 057 */     java.lang.String value_3 = null;\n",
      "/* 058 */     isNull_3 = false;\n",
      "/* 059 */     if (!isNull_3) {\n",
      "/* 060 */\n",
      "/* 061 */       Object funcResult_1 = null;\n",
      "/* 062 */       funcResult_1 = value_4.toString();\n",
      "/* 063 */       value_3 = (java.lang.String) funcResult_1;\n",
      "/* 064 */\n",
      "/* 065 */     }\n",
      "/* 066 */     if (isNull_3) {\n",
      "/* 067 */       values_0[1] = null;\n",
      "/* 068 */     } else {\n",
      "/* 069 */       values_0[1] = value_3;\n",
      "/* 070 */     }\n",
      "/* 071 */\n",
      "/* 072 */     UTF8String value_6 = i.getUTF8String(2);\n",
      "/* 073 */     boolean isNull_5 = true;\n",
      "/* 074 */     java.lang.String value_5 = null;\n",
      "/* 075 */     isNull_5 = false;\n",
      "/* 076 */     if (!isNull_5) {\n",
      "/* 077 */\n",
      "/* 078 */       Object funcResult_2 = null;\n",
      "/* 079 */       funcResult_2 = value_6.toString();\n",
      "/* 080 */       value_5 = (java.lang.String) funcResult_2;\n",
      "/* 081 */\n",
      "/* 082 */     }\n",
      "/* 083 */     if (isNull_5) {\n",
      "/* 084 */       values_0[2] = null;\n",
      "/* 085 */     } else {\n",
      "/* 086 */       values_0[2] = value_5;\n",
      "/* 087 */     }\n",
      "/* 088 */\n",
      "/* 089 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));\n",
      "/* 090 */\n",
      "/* 091 */     return value_0;\n",
      "/* 092 */   }\n",
      "/* 093 */\n",
      "/* 094 */ }\n",
      "\n",
      "25/01/28 14:40:27 INFO CodeGenerator: Code generated in 5.789334 ms\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ratingsDF' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m moviesDF\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRatings Dataset:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m \u001b[43mratingsDF\u001b[49m\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Join ratings with movies on movieId\u001b[39;00m\n\u001b[1;32m     29\u001b[0m ratings \u001b[38;5;241m=\u001b[39m ratingsDF\u001b[38;5;241m.\u001b[39mjoin(moviesDF, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmovieId\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ratingsDF' is not defined"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/28 14:40:44 DEBUG ExecutorMetricsPoller: removing (1, 0) from stageTCMP\n",
      "25/01/28 14:40:44 DEBUG ExecutorMetricsPoller: removing (2, 0) from stageTCMP\n",
      "25/01/28 14:40:44 DEBUG ExecutorMetricsPoller: removing (0, 0) from stageTCMP\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.sql.functions import col, explode\n",
    "\n",
    "# Initialize Spark session with resource optimization\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Collaborative Filtering\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .config(\"spark.default.parallelism\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to DEBUG to capture detailed logs\n",
    "spark.sparkContext.setLogLevel(\"DEBUG\")\n",
    "\n",
    "# Load movies and ratings data (Limit the data for testing purposes)\n",
    "moviesDF = spark.read.options(header=\"True\", inferSchema=\"True\").csv(\"./data/movies.csv\").limit(1000)\n",
    "ratingsDF = spark.read.options(header=\"True\", inferSchema=\"True\").csv(\"./data/rating.csv\")\n",
    "\n",
    "# Display the dataframes to ensure they are loaded correctly\n",
    "print(\"Movies Dataset:\")\n",
    "moviesDF.show(5)\n",
    "print(\"Ratings Dataset:\")\n",
    "ratingsDF.show(5)\n",
    "\n",
    "# Join ratings with movies on movieId\n",
    "ratings = ratingsDF.join(moviesDF, 'movieId', 'left')\n",
    "\n",
    "# Split the data into training and testing sets (Use a smaller split for local testing)\n",
    "(train, test) = ratings.randomSplit([0.8, 0.2])\n",
    "\n",
    "# Print the count of rows in the training and test datasets\n",
    "print(f\"Training Data Count: {train.count()}\")\n",
    "train.show(5)\n",
    "print(f\"Test Data Count: {test.count()}\")\n",
    "test.show(5)\n",
    "\n",
    "# Initialize the ALS model with user and item column names\n",
    "als = ALS(userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", \n",
    "          nonnegative=True, implicitPrefs=False, coldStartStrategy=\"drop\")\n",
    "\n",
    "# Set up parameter grid for cross-validation\n",
    "param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(als.rank, [10, 50, 100, 150]) \\\n",
    "            .addGrid(als.regParam, [.01, .05, .1, .15]) \\\n",
    "            .build()\n",
    "\n",
    "\n",
    "# Initialize evaluator for RMSE\n",
    "evaluator = RegressionEvaluator(\n",
    "    metricName=\"rmse\", \n",
    "    labelCol=\"rating\", \n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "# Initialize CrossValidator\n",
    "cv = CrossValidator(estimator=als, estimatorParamMaps=param_grid, evaluator=evaluator, numFolds=3)\n",
    "\n",
    "# COMMAND ----------\n",
    "# Fit the model using cross-validation\n",
    "model = cv.fit(train)\n",
    "\n",
    "# Get the best model from cross-validation\n",
    "best_model = model.bestModel\n",
    "\n",
    "# Transform the test set using the best model\n",
    "test_predictions = best_model.transform(test)\n",
    "\n",
    "# Evaluate the RMSE on the test set\n",
    "RMSE = evaluator.evaluate(test_predictions)\n",
    "print(f\"Test RMSE: {RMSE}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "# Get recommendations for all users (Limit the number of recommendations for testing)\n",
    "recommendations = best_model.recommendForAllUsers(3)\n",
    "\n",
    "# COMMAND ----------\n",
    "# Display the recommendations dataframe\n",
    "print(\"Recommendations for All Users:\")\n",
    "recommendations.show(5)\n",
    "\n",
    "# COMMAND ----------\n",
    "# Explode the recommendations array and select the relevant columns\n",
    "df2 = recommendations.withColumn(\"movieid_rating\", explode(\"recommendations\"))\n",
    "\n",
    "# Display the exploded dataframe with selected columns\n",
    "df2.select(\n",
    "    \"userId\", \n",
    "    col(\"movieid_rating.movieId\").alias(\"movieId\"), \n",
    "    col(\"movieid_rating.rating\").alias(\"rating\")\n",
    ").show(5)\n",
    "\n",
    "# COMMAND ----------\n",
    "# Stop the Spark session after the job is complete\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
