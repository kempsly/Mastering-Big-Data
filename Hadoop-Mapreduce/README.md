---

# Hadoop MapReduce

This project demonstrates the implementation and execution of MapReduce jobs in a Hadoop ecosystem. The code provided shows how to process large datasets using the MapReduce programming model, leveraging Hadoop's distributed computing framework.

## Table of Contents

- [Project Overview](#project-overview)
- [Requirements](#requirements)
- [Setup](#setup)
- [Running the Code](#running-the-code)
- [MapReduce Example](#mapreduce-example)
- [Common Issues](#common-issues)
- [License](#license)

## Project Overview

This repository contains implementations of **MapReduce** jobs in the **Hadoop ecosystem**. These jobs are used to process large datasets by dividing the tasks into smaller sub-tasks, which are then executed in parallel across the distributed Hadoop cluster.

The main components of the Hadoop MapReduce job are:
- **Mapper**: Processes input data and outputs intermediate key-value pairs.
- **Reducer**: Aggregates the intermediate data generated by the mapper.
- **Driver**: Configures and runs the MapReduce job.

## Requirements

- **Hadoop**: Make sure Hadoop is installed and configured correctly in your environment. You can check the version using:
  
  ```bash
  hadoop version
  ```
  
- **Java**: The job must be written in Java. Ensure that the required version of Java is installed on your machine. Typically, Hadoop requires Java 8 or higher.
  
- **HDFS**: Hadoop Distributed File System (HDFS) should be running to store and retrieve the input/output data files for MapReduce jobs.

- **Maven** (optional for building the project): If you’re using Maven for dependency management, make sure it is installed on your machine.

## Setup

### 1. Clone the Repository

First, clone this repository to your local machine:

```bash
git clone https://github.com/kempsly/Mastering-Big-Data/Hadoop-Mapreduce.git
cd Hadoop-Mapreduce
```

### 2. Install Hadoop

Ensure you have Hadoop installed. If you don’t have it, follow these steps:

- Download Hadoop from the official Apache website: [Apache Hadoop Downloads](http://hadoop.apache.org/releases.html).
- Follow the installation guide: [Hadoop Installation Guide](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html).

### 3. Configure Hadoop

After installing Hadoop, make sure to configure it properly by setting the environment variables and configuring `core-site.xml`, `hdfs-site.xml`, and `mapred-site.xml` as required.

```bash
export HADOOP_HOME=/path/to/hadoop
export JAVA_HOME=/path/to/java
```

Ensure that your HDFS is running and Hadoop daemons are configured to run in pseudo-distributed or fully-distributed mode.

### 4. Build the Project (If Using Maven)

If you're using Maven for dependency management, run the following command to build the project:

```bash
mvn clean install
```

Alternatively, you can manually compile your Java files using `javac`.

## Running the Code

### 1. Prepare Input Data

Make sure to have input data available in HDFS. For example, you can upload a sample file:

```bash
hdfs dfs -put input.txt /user/hadoop/input/
```

### 2. Run the MapReduce Job

To run the MapReduce job, use the `hadoop jar` command:

```bash
hadoop jar /path/to/your/hadoop-mapreduce.jar com.example.YourMainClass /user/hadoop/input /user/hadoop/output
```

Replace `/path/to/your/hadoop-mapreduce.jar` with the location of your compiled `.jar` file, and update the class name (`com.example.YourMainClass`) accordingly.

### 3. View the Output

After the job completes, you can check the output in HDFS:

```bash
hdfs dfs -cat /user/hadoop/output/part-*
```

### 4. (Optional) Monitor the Job

You can monitor your job’s progress and logs through the Hadoop Web UI, typically accessible at `http://localhost:50070/`.

## MapReduce Example

### Example: Word Count

This project contains an example **Word Count** MapReduce job that counts the frequency of each word in a text file.

### Mapper Class

The mapper reads each line of the input, splits it into words, and outputs the word as the key and the count (1) as the value.

### Reducer Class

The reducer aggregates the counts for each word and outputs the final word count.

### Sample Input

```
Hello world
Hello Hadoop
```

### Sample Output

```
Hello    2
world    1
Hadoop   1
```

## Common Issues

### 1. **Error: JAVA_HOME not set**

Make sure that the `JAVA_HOME` environment variable is correctly set and points to your JDK installation.

### 2. **HDFS Permissions Issues**

Ensure that you have the correct permissions to access the HDFS directories. If you encounter permission issues, use:

```bash
hdfs dfs -chmod -R 777 /user/hadoop/input
```

### 3. **Job Failures**

If your MapReduce job fails, check the job logs for any exceptions. This can be done through the Hadoop Web UI or by reviewing the logs located in the `logs` directory of Hadoop.

## License

This project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details.

---
